{"pages":[{"title":"","text":"About me I am Shu-Hsien Cho (å“æ›¸è³¢), a third-year Ph.D. student studying Biostatistics at University of Texas MD Anderson UTHealth Graduate School of Biomedical Sciences. Before pursuing this Ph.D. degree, I was a research assistant under the supervision of Dr. Yen-Tsung Huang at Institute of Statistical Science (ISS), Academia Sinica. I received my Bachelor degree with double major in Statistics and Economics at National Chengchi University in Jan, 2017. My research interest is genomic data analysis and survival analysis. In addition, I am also interested in causal mediation analysis, and the interaction of survival analysis with other fields. In my research spare time, I am also interested in learning applicable methodologies for baseball data analysis (e.g., bayesian approaches, spatial data, etc.) I also create a baseball data analysis blog (in Mandarin) with my friend. Outside academics, I love baseball, weight training, and photography. Also, I enjoy cooking, wine/beer/whiskey tasting, watching sports game, reading, and sometimes playing contract bridge. In addition to being an outstanding researcher, I hope to keep asking questions and fulfill my curiosity to this world. CV","link":"/about/index.html"},{"title":"","text":"å”è‰ºæ˜• ææ² æä¸€æ¡ gakki å›¾ç‰‡æœé›†äºäº’è”ç½‘ï¼Œä¾µæƒè¯·ç•™è¨€ï¼Œé©¬ä¸Šå¤„ç†ğŸ˜Šã€‚","link":"/album/index.html"},{"title":"","text":"&lt;div class=&quot;music-player&quot;&gt; &lt;div class=&quot;d-title&quot;&gt; &lt;i class=&quot;fa fa-music&quot;&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;å¬å¬éŸ³ä¹ &lt;/div&gt; &lt;br/&gt; &lt;/div&gt; &lt;div id=&quot;musicarea&quot;&gt; &lt;div class=&quot;music&quot;&gt;&lt;/div&gt; &lt;p id=&quot;p_message&quot;&gt;&lt;span id=&quot;music_story_message&quot; class=&quot;span_animation&quot;&gt;&lt;/span&gt;&lt;/p&gt; &lt;br/&gt; &lt;ul id=&quot;musiclist&quot;&gt;&lt;/ul&gt; &lt;br/&gt; &lt;div id=&quot;desc&quot;&gt;&lt;/div&gt; &lt;/div&gt; éŸ³ä¹æ’­æ”¾å™¨ç”±mePlayeræä¾›ï¼Œå¸ƒå±€å‚ç…§ç½‘å‹åšå®¢æ‰€ä½œï¼Œæ„Ÿè°¢ä½œè€…çš„è¾›å‹¤ä»˜å‡ºã€‚æ›´å¤šéŸ³ä¹åˆ†äº«è¯·æŸ¥çœ‹æ­Œå•ã€‚ &lt;div class=&quot;d-title&quot;&gt; &lt;i class=&quot;fa fa-video-camera&quot;&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;çœ‹çœ‹è§†é¢‘ &lt;/div&gt; &lt;br/&gt; &lt;p class=&quot;hits&quot;&gt;-&gt;ç‚¹å‡»ä»¥ä¸‹æ¡ç›®å¼€å§‹æ’­æ”¾è§†é¢‘,å‘ä¸‹æ»‘åŠ¨æŸ¥çœ‹æ›´å¤š&lt;-&lt;/p&gt; &lt;div id=&quot;video-list&quot;&gt;&lt;/div&gt; &lt;br/&gt; &lt;div id=&quot;dplayer&quot;&gt;&lt;br/&gt;&lt;/div&gt;","link":"/media/index.html"},{"title":"","text":"ç”³è¯·å‹é“¾é¡»çŸ¥ åŸåˆ™ä¸Šåªå’ŒæŠ€æœ¯ç±»åšå®¢äº¤æ¢ï¼Œä½†ä¸åŒ…æ‹¬å«æœ‰å’Œè‰²æƒ…ã€æš´åŠ›ã€æ”¿æ²»æ•æ„Ÿçš„ç½‘ç«™ã€‚ ä¸å’Œå‰½çªƒã€ä¾µæƒã€æ— è¯šä¿¡çš„ç½‘ç«™äº¤æ¢ï¼Œä¼˜å…ˆå’Œå…·æœ‰åŸåˆ›ä½œå“çš„ç½‘ç«™äº¤æ¢ã€‚ ç”³è¯·è¯·æä¾›ï¼šç«™ç‚¹åç§°ã€ç«™ç‚¹é“¾æ¥ã€ç«™ç‚¹æè¿°ã€logoæˆ–å¤´åƒï¼ˆä¸è¦è®¾ç½®é˜²ç›—é“¾ï¼‰ã€‚ æ’åä¸åˆ†å…ˆåï¼Œåˆ·æ–°åé‡æ’ï¼Œæ›´æ–°ä¿¡æ¯åè¯·ç•™è¨€å‘ŠçŸ¥ã€‚ ä¼šå®šæœŸæ¸…ç†å¾ˆä¹…å¾ˆä¹…ä¸æ›´æ–°çš„ã€ä¸ç¬¦åˆè¦æ±‚çš„å‹é“¾ï¼Œä¸å†å¦è¡Œé€šçŸ¥ã€‚ æœ¬ç«™ä¸å­˜å‚¨å‹é“¾å›¾ç‰‡ï¼Œå¦‚æœå‹é“¾å›¾ç‰‡æ¢äº†æ— æ³•æ›´æ–°ã€‚å›¾ç‰‡è£‚äº†çš„ä¼šæ›¿æ¢æˆé»˜è®¤å›¾ï¼Œéœ€è¦æ›´æ¢çš„è¯·ç•™è¨€å‘ŠçŸ¥ã€‚ æœ¬ç«™å‹é“¾ä¿¡æ¯å¦‚ä¸‹ï¼Œç”³è¯·å‹é“¾å‰è¯·å…ˆæ·»åŠ æœ¬ç«™ä¿¡æ¯ï¼š ç½‘ç«™å›¾æ ‡ï¼šhttps://removeif.github.io/images/avatar.jpg ç½‘ç«™åç§°ï¼šè¾£æ¤’ã®é…± ç½‘ç«™åœ°å€ï¼šhttps://removeif.github.io ç½‘ç«™ç®€ä»‹ï¼šåç«¯å¼€å‘ï¼ŒæŠ€æœ¯åˆ†äº« åŠ è½½ä¸­ï¼Œç¨ç­‰å‡ ç§’...","link":"/friend/index.html"},{"title":"","text":"Please leave comments if you have any question or idea about the post!","link":"/message/index.html"},{"title":"éŸ³ä¹æ­Œå•æ”¶è—","text":"--- æ¸©é¦¨æç¤ºï¼šé€‰æ‹©å–œæ¬¢çš„éŸ³ä¹åŒå‡»æ’­æ”¾ï¼Œç”±äºç‰ˆæƒåŸå› éƒ¨åˆ†ä¸èƒ½æ’­æ”¾ã€‚å¦‚æœå–œæ¬¢æ­Œå•æ”¶è—ä¸€ä¸‹ï¼Œå»ç½‘æ˜“äº‘éƒ½èƒ½æ’­æ”¾å“Ÿï¼","link":"/music/index.html"}],"posts":[{"title":"MD Anderson GSBSç¬¬ä¸€å­¸æœŸå›é¡§","text":"åœ¨MD Andersonè‡ªé–‰çš„ç¬¬ä¸€å­¸æœŸå³å°‡çµæŸï¼Œåœ¨åƒèˆ‡13-16è™Ÿä¹‹é–“çš„ICSA Applied Statistics Symposiumsä¹‹å¤–ï¼Œé‚„å¾—åœ¨16è™Ÿè€ƒRiceçµ±è¨ˆæ¨è«–ä¸€çš„æœŸæœ«è€ƒï¼Œæ¥ä¸‹ä¾†å°±æ˜¯å¯’å‡çš„å°ä¼‘æ¯èˆ‡resetï¼Œåœ¨å¿µæ›¸ä¹‹é¤˜æƒ³åˆ†äº«ä¸€ä¸‹å¯èƒ½æ˜¯åœ¨é€™å­¸æ ¡æœ€ç‰¹åˆ¥çš„ä¸€å­¸æœŸï¼Œä¹Ÿçµ¦è‡ªå·±ä¸€é»å›é¡§ã€‚ Core Course é€™é–€èª²å¯ä»¥èªªæ˜¯GSBSæœ€é‡è¦çš„èª²ç¨‹ï¼Œä¹Ÿåƒæ‰æˆ‘æœ€å¤šæ™‚é–“ï¼Œæ¯å¤©æ—©ä¸Šéƒ½éœ€è¦ä¸Š2.5å°æ™‚çš„èª²å¤–ï¼Œæ¯é€±ä¹Ÿéƒ½æœ‰2å€‹ä½œæ¥­èˆ‡1å€‹journal club/debateç­‰ç­‰ã€‚å› ç‚ºæ˜¯umbrella programçš„ç·£æ•…æ¯ä¸€é€±éƒ½æœ‰ä¸ä¸€æ¨£çš„ä¸»é¡Œï¼Œæˆ‘æƒ³å¾ä¸€å€‹å¿µçµ±è¨ˆ/Quantitative Scienceçš„è§’åº¦ä¾†çœ‹é€™é–€èª²ã€‚ å„ªé» å› ç‚ºè¦coveræ¯é€±çš„ä¸»é¡Œï¼Œé€™é–€èª²æ‰¾ä¾†MD Anderson / UT Medical School / BCM ç­‰åœ°çš„è€å¸«æ”¯æ´é–‹èª²ï¼Œå°æˆ‘ä¾†èªªæ˜¯å¾ˆæ–°çš„ä¸Šèª²é«”é©—ã€‚è€Œä¸”å› ç‚ºä¸å°‘è€å¸«éƒ½æƒ³æ”¶å­¸ç”Ÿï¼Œå› æ­¤ä¸Šèª²ä¹Ÿå¯èƒ½æ›´èªçœŸï¼Œæˆ–è€…æœƒæ¨éŠ·è‡ªå·±ã€‚æ­¤å¤–æ¯é€±ä¸€ä¸»é¡Œé€ æˆå…§å®¹ä¸æœƒéåˆ†æ·±å…¥ï¼Œç›¸å°ä¾†èªªå‰‡æ˜¯æ›´å¿«çš„ç¯€å¥ï¼Œå°æ–¼æ¯«ç„¡åŸºç¤çš„æˆ‘é›–ç„¶ä»å±¬è¾›è‹¦ï¼Œä½†æ˜¯å¯ä»¥æƒ³åƒè‚¯å®šæ¯”ç´”ç²¹çš„PhD levelç”Ÿç‰©èª²æ›´æ˜“ä¸Šæ‰‹ã€‚åŠ ä¸Šé€™å­¸æœŸå…¶å¯¦ä¹Ÿæœ‰ä¸å°‘å…§å®¹æ˜¯æ‹¿COVIDä¾†ä½œç‚ºèˆ‰ä¾‹ç”šè‡³æ˜¯ç•¶é€±çš„journal club / grant proposalä¸»é¡Œï¼Œå°æ–¼ä¸»é¡Œçš„æŠ•å…¥æ›´æœ‰æ„Ÿå—ï¼ˆä½†å¸Œæœ›æˆ‘éäº†é€™å­¸æœŸä¸æœƒçœŸçš„å¿˜å…‰å…‰å°±å¥½ï¼‰ã€‚æ­¤å¤–é€™å ‚èª²çš„ä¸»è»¸ä¹Ÿå°bioinfo/statistical geneticsçš„ç ”ç©¶æ–¹å‘æ›´ç‚ºå‹å¥½ï¼ Translational studies é€™é–€èª²å…¶å¯¦æŸå€‹ç¨‹åº¦ä¸Šè­‰å¯¦äº†é€™å€‹å­¸æ ¡æ˜¯ç›¸å°æ“…é•·è½‰è­¯ç ”ç©¶çš„ï¼Œå¾ˆå¤šæ™‚å€™æˆ‘å€‘éƒ½åœ¨çœ‹ç™Œç—‡ç›¸é—œçš„target geneä»¥åŠmechanism/machineryï¼Œæˆ–è€…è¨è«–ç‰¹å®šçš„pathwayï¼Œæœ‰æ™‚å€™æœƒè¦æ±‚æˆ‘å€‘åŸºæ–¼ä»¥ä¸Šçš„è³‡è¨Šè¨­æƒ³ä¸€äº›æœ‰ç³»çµ±çš„å°é¼ å¯¦é©—ç­‰ç­‰ï¼Œé€™ç›¸ç•¶ç¬¦åˆtranslational studyçš„ç²¾ç¥ï¼Œä¹Ÿè®“æˆ‘äº†è§£åˆ°å°‡ä¾†å¯èƒ½çš„åˆä½œå°è±¡æ˜¯å¦‚ä½•å•å‡ºç§‘å­¸å•é¡Œï¼Œåªæ˜¯ä¸¦ä¸æœƒå»¶ä¼¸åˆ°å°æ–¼çµ±è¨ˆå­¸å®¶ä¾†èªªæ›´å®¹æ˜“å”åŠ©çš„Phase 2ä»¥å¾Œçš„studyï¼Œæ›´å¤šåƒæ˜¯å¾benchçš„ç ”ç©¶è©¦åœ–å»¶ä¼¸åˆ°ç—…äººçš„ç ”ç©¶ã€‚ ç¼ºé» ä»¥ç”Ÿç‰©çµ±è¨ˆçš„è§’åº¦ä¾†çœ‹ï¼Œé€™é–€èª²çš„å¹«åŠ©å¾ˆæœ‰é™ï¼Œåœ¨å‰ä¸€å€‹æœˆçš„èª²ç¨‹å¯ä»¥è¦–ç‚ºGenetic Epidemiologyçš„ç”Ÿç‰©èƒŒæ™¯çŸ¥è­˜ã€‚æ¯é€±çš„journal club/æŒ‡å®šé–±è®€æ–‡æœ¬ä¹Ÿéƒ½æœ‰åŒ…å«å¹¾ç¨®åŸºæœ¬çš„æª¢å®šæˆ–è€…å­˜æ´»åˆ†æç”šè‡³æ˜¯è¦–è¦ºåŒ–ï¼Œè€Œé€™ä¹Ÿæ˜¯biostat/bioinfoèª²å ‚éƒ½æœ‰æåŠçš„ï¼Œåªæ˜¯æ¯é€±ä¸»é¡Œè·Ÿçµ±è¨ˆçš„æ•´åˆæ•ˆæœä¸ä½³ï¼Œé€™å¾æ¯é€±çš„biostatä½œæ¥­åŒå­¸çš„å›ç­”å¯ä»¥æ„Ÿå—åˆ°ã€‚ æ­¤å¤–åœ¨MDACCï¼Œä¸å°‘çš„labéœ€è¦è‡ªå·±åšsingle cell sequencingï¼Œä¸”è½‰è­¯é†«å­¸æœ¬èº«æ‡‰è©²ä¹Ÿéœ€è¦å¤šå­¸ç§‘çš„äº’ç›¸æ”¯æ´ï¼Œä½†æ˜¯é€™é–€èª²ä¸¦æ²’æœ‰å¸¶åˆ°èˆ‡dry labåˆä½œçš„å¿…è¦æ€§ï¼Œå¤šæ•¸åœ¨å°çµ„æ´»å‹•ä¸Šçš„å…§å®¹ä¹Ÿéƒ½æ˜¯ä»¥ç”Ÿç‰©ç§‘å­¸å…§å®¹ç‚ºä¸»ï¼Œä¸¦æ²’æœ‰è®“ç”Ÿç‰©èƒŒæ™¯çš„å­¸ç”Ÿå»å˜—è©¦å•ä»€éº¼æ™‚å€™éœ€è¦é‡åŒ–ï¼Œæ–¼æ˜¯QSåœ¨é€™å ‚èª²ä¸­æ›´åƒæ˜¯å¦å¤–ä¸€ç¨®çªå…€çš„å­˜åœ¨ã€‚ æœ€å¾Œå‰‡æ˜¯éæˆ°ä¹‹ç½ª--äº’å‹•ï¼Œé€™å—åˆ¶æ–¼ç•¶å‰ç–«æƒ…ï¼Œé€™é–€èª²ç•¢ç«Ÿæ¯å¤©éƒ½è¦çœ‹åˆ°ä¸€æ¨£çš„äºº2.5å°æ™‚åˆ°ä¸­åˆï¼Œæ˜¯ä¸€å€‹å¾ˆå¥½è®“åŒå­¸äº’ç›¸äº¤æœ‹å‹çš„æ©Ÿæœƒï¼Œç”šè‡³å»¶ä¼¸åˆ°æ—¥å¾Œçš„åˆä½œï¼Œåªæ˜¯ç·šä¸Šä¸Šèª²å¤§å¤§é˜»ç¤™äº†å¯èƒ½æ€§ã€‚ Rotation é€™æ‡‰è©²æ˜¯å°æ–¼çµ±è¨ˆå­¸ç”Ÿä¾†æˆ‘å€‘å­¸æ ¡æ¯”è¼ƒå¹¸ç¦çš„é»ï¼Œç•¢ç«Ÿå¾ˆå°‘æœ‰äººå¯ä»¥åœ¨ä¸€å¹´ç´šå°±é–‹å§‹åšæ±è¥¿ã€‚ç›¸å°æ–¼ä¸€å€‹ç¦®æ‹œè¦äº”å€‹åŠå¤©çš„core courseï¼Œé€™ä»¶äº‹å°æˆ‘ä¾†èªªæ›´ç†Ÿæ‚‰ã€‚åšäº†ä¸€äº›å­˜æ´»åˆ†æçš„variable selectionåŒæ™‚ä¼°è¨ˆçš„å˜—è©¦èˆ‡å¯¦ç¾ï¼Œä¸¦ä¸”è©¦åœ–ç”¨åœ¨UK biobankä¸Šï¼Œæ¥ä¸‹ä¾†å¸Œæœ›å¯’å‡å¯ä»¥å®Œæˆå¤§éƒ¨åˆ†çš„å·¥ä½œä¸¦ä¸”é–‹å§‹å¯«Technical reportï¼Œåªæ˜¯æƒ³è¦åŒæ™‚åœ¨ç§åº•ä¸‹å¤šåšä¸€é»æ±è¥¿çš„è¨ˆåŠƒä¸¦æ²’æœ‰å¾ˆæˆåŠŸï¼Œå¯èƒ½åœ¨ä¿®èª²ä¸Šå¤šèŠ±äº†ä¸å°‘æ™‚é–“ï¼Œå¸Œæœ›ä¸‹å€‹å­¸æœŸåœ¨ä¿®èª²è·Ÿåšç ”ç©¶ä¸Šå¯ä»¥å…¼é¡§ï¼Œä¸¦ä¸”ä¸‹å­¸æœŸçš„rotationæ˜¯æœ‰æ©Ÿæœƒå¯ä»¥è‡ªå·±ç™¼å±•ä¸€äº›å…ˆå‰èˆ‡è€å¸«è¨è«–éçš„æƒ³æ³•ï¼Œè®“æˆ‘æ›´åŠ æœŸå¾…ï¼ä¹Ÿè¦æ„Ÿè¬Ryançµ¦äº†è »å¤šæƒ³æ³•è·Ÿå»ºè­°ï¼Œåœ¨é€™å€‹ç‰¹æ®Šçš„æƒ…å½¢èˆ‡å…¨æ–°çš„ç’°å¢ƒä¸­è®“æˆ‘ä»ç„¶é‚„ç®—é †åˆ©åœ°æ´»éé€™å­¸æœŸã€‚ å¦å¤–ï¼Œåœ¨é€™è£¡ä¼¼ä¹ä¸ç”¨è€ƒæ…®ã€Œé€™çœŸçš„ç”¨å¾—åˆ°å—ï¼Ÿã€ã€Œæ‡‰ç”¨ç«¯åœ¨æ„å—ï¼Ÿã€ï¼Œæ›´å¤šæ™‚å€™å°±æ˜¯å› ç‚ºæœ‰é€™äº›è³‡æ–™èˆ‡ç¾è±¡æ‰æœƒå†’å‡ºé€™äº›å•é¡Œï¼ˆæˆ–æ›´æº–ç¢ºçš„èªªï¼Œæœ‰é€™äº›Grantï¼‰è€Œèƒ½å¤ æ›´ç›´æ¥æ„Ÿå—åˆ°è©²è§£æ±ºçš„å•é¡Œã€‚å› æ­¤æˆ‘çŒœæƒ³å¦‚æœæ˜¯å³æˆ°åŠ›åˆ°MD Andersonä¾†å”¸æ›¸å¯èƒ½æœƒå¾ˆæœ‰æ”¶ç©«ï¼Œé€™æ¨£åœ¨ä¸€é–‹å§‹å°±å¯ä»¥æ±ºå®šå¥½ç™¼å±•çš„èµ°å‘ï¼Œæœ‰ç ”ç©¶ç¶“é©—çš„å¹¾å€‹åŒå­¸ä¹Ÿæ˜¯å¾ˆå¿«é€Ÿåœ°å°±æŠŠè‡ªå·±çš„rotationå®šå¥½ï¼Œé€™æ˜¯å°è€å¸«æˆ–è€…å°å­¸ç”Ÿéƒ½å¾ˆæœ‰æ•ˆç‡çš„åšæ³•ï¼Œä¹Ÿå¯èƒ½æ˜¯ä»–å€‘åœ¨å…¥å­¸å¯©è³‡æ–™çš„æ™‚å€™æœƒçœ‹Research statementçš„ç·£æ•…ï¼Œå¸Œæœ›æ—¥å¾Œé‚„èƒ½å›ä¾†é©—è­‰é€™å€‹çŒœæƒ³XD å…¶ä»– å¤šè™§é€™è£¡é¸èª²è‡ªç”±ï¼Œæˆ‘èƒ½è·¨åˆ°Riceå»ä¸Šä¸€äº›æ›´æ·±å…¥çš„èª²ç¨‹é‚„æ˜¯ä¸éŒ¯ï¼ˆè€Œä¸”é‚„ä¸ç”¨å¤šä»˜éŒ¢ï¼‰ï¼Œé€™æ¨£å¯ä»¥è®“è¨“ç·´æ›´å¤šå…ƒåˆåŒæ™‚æœ‰æ·±åº¦ï¼Œåªæ˜¯é‚„æœ‰éƒ¨åˆ†éœ€è¦æ”¹å–„ï¼Œç‰¹åˆ¥æ˜¯WFHçš„ç¯€å¥æ²’æœ‰å¾ˆé †åˆ©ï¼Œä»¥åŠä¸‹å­¸æœŸç›¸å°æ›´é‡çš„èª²ç¨‹è² æ“”ï¼Œé‚„æœ‰å¸Œæœ›æŠŠä¸‹å…©å€‹rotationåšå¾—ç¬¦åˆæœŸå¾…ï¼Œæ‡‰è©²æœƒæ˜¯æ›´å¤§çš„æŒ‘æˆ°ã€‚","link":"/posts/mdacc20fall/"},{"title":"Encrypted article test","text":"Please try the password 123456. b0929fd6bf1e3d692d23808e2d9f7ec461bbd2261c4a3c77cb0211ef6876a91ae820900900a1494a3b16f0ca7e84bc4e6aa9b17c7c0b1b019f603428caf046f5be8bb4efe97ab9c32919c4cedd332993ff52721aaefabd74ff4d9c93fad613395e19a719244d40f7255e64e401c3f6f1d5e326c481e0571adf76805cd4ce1b1c834fa2b528cd08f2e88d0e7f9de08132014a5d540b0983d4c40bddd66834d6d516518e5d150ed53941a28888c164ddf5ac1a3e5f9212838bc594629bd53283d70374594606c756d00f864a3c90c6b7f5f90e14e7f0e6a01ad0eb9b8074bda3a895b488face5f0230051f081b70e8ad566184d44bce3447b2e3d517ee8b6bd4279a53a400f49ba10293aa179eb2eeca7f33999e658357c140eb5bbff6afa197c8c73147e37d38eb22214a9fea7f79161254b8b7cb49214abb8d51fbc249276a4e3b0eaa401b75b04008d96e4a2ce88dfa389799cfc7e5bc8e43f944de4005d05d889d8d8100990ceefc88f5c1d2df3dd370fbf76c942f27a5b40d6aac58be8a3d30e36f80627e721a9d6a5c46d2573bd8670ba8ff67f73509c5c72dba5a21939f358bf84644f2ea978fd7f12f25e4f7b0155674132de5dfa85ed4985b9b0168364611d70731b466787a9515e5696c20261b39fec546203e473cd58d9a2f72c677b8d1c7d175d7cc91dcf4a07767b6912b21fbbc626b2f68ba6379d1a787000576cb7a8b22c7dd80c9b1a2e73f5e3c8bc83bf969adaa1b3b1c586e68b69bb9763eddcf50720a4473d5e8b797e0360724b29674d4876211b549ec961eb193a4f8a09952db854d72c339ac18571e1ee9eb6b18300d853ae15f473ac60dd7d607ecf40bc0b18a5b85b8b68f8e2f62f5bb71ec27919b426d70b6ad249f8bea92af12ad","link":"/posts/7e709353/"},{"title":"Statistical Inference Note","text":"This lecture note is based on Rice STAT 532: Foundations of Statistical Inference I and Rice STAT 533: Foundations of Statistical Inference II. All the lecture notes will provide an advanced introduction to the mathematical theory of statistics, including comprehensive measure-theoretic probability, common families of distributions, statistical decision theory, statistics, and estimation. Chapter 1: Measure Space File This chapter discusses measures and their definition, construction of functions and induced measures, integration and differentiation, and conditional expectation/distributions. Chapter 2: Probability Measures on Euclidean Spaces File This chapter discusses the detailed properties of probability models on Euclidean spaces, moment inequalities, generating functions, distribution families, and order statistics.","link":"/posts/statinfnote/"},{"title":"Inference on Selected Subgroups in Clinical Trials","text":"This note is based on the X. Guo and X. He (2020), Inference on Selected Subgroups in Clinical Trials, Journal of American Statistical Association Subgroup Analyses and Selection bias Overview Subgroup analyses figure out promising treatments to some subgroups. In practice, subgroup analyses consists of two steps: subgroup identification and subgroup confirmation. In the identification step, researchers looks for the best selected subgroup which comes from domain considerations or data-driven. The existing data-driven identification methods are mostly machine-learning based or model based. The confirmation step provides an inference for possibly additional trials on the promising subgroup. Generally, The inference to the selected subgroup may lead to an overly optimistic evaluation, which is called subgroup selection bias. This kind of bias is originated from the selection mechanism of \\(\\max\\limits_{i=1,\\cdots,k}(\\beta_i)\\). Some of the existing methods for subgroup confirmation might be conservative, without theoretical justification, or model-dependent. Thus, this paper proposes a (data-adaptive) de-biasing bootstrap estimate and a valid one-sided confidence bound. Procedure For predifined subgroups, the interested estimators are the \\(\\beta_s\\) (the true effect size of the subgroup with the largest estimated effect) and \\(\\beta_{\\max}\\) (the largest true subgroup effect size). The procedure is: Obtain the largest modified bootstrapped subgroup effect \\(\\beta^*_{\\max,\\text{modified}}=\\max\\limits_{i=1,\\cdots,k}(\\beta^*_i+d_i)\\) with modification \\(d_i=(1-n^{r-0.5})(\\hat\\beta_{\\max}-\\hat\\beta_i)\\) Construct the bootstrap confidence interval \\(\\hat\\beta_{\\max}\\pm c_\\alpha/\\sqrt{n}\\) with \\(c_{\\alpha}=quantile(\\sqrt{n}(\\max\\limits_{i=1,\\cdots,k}(\\beta^*_{i,b}+d_i)-\\hat\\beta_{\\max}),1-\\alpha)\\) (asymptotically sharp interval for \\(\\beta_s\\)). Derive the bias-reduced estimator \\(\\hat\\beta_{\\max,\\text{reduced}}=\\hat\\beta_{\\max}-E^*[\\beta^*_{\\max,\\text{modified}}-\\hat\\beta_{\\max}]\\) For data-adaptive \\(r\\), tune \\(r\\) with cross validation and bias-reduced estimator. While considering the post-hoc subgroup identification, the procedure is similar to the predefined subgroup identification but no data-adaptive selection for \\(r\\). Possible Extention This method is useful for homogeneous subgroups, because the subgroup selection bias in the homogeneous subgroups might be larger than one in the heterogeneous subgroups. When the subgroup effects are heterogeneous, the naive approach of subgroup identification provide an efficient enough result with less empirical bias. However, the tradeoff among the tuning parameter \\(r\\), heterogeneity, and number of subgroups has not been discussed. With the simulation provided in this paper, the bias and coverage get worse when the number of subgroups or \\(r\\) gets large. When \\(r\\) gets large, it means that the effect modification \\(d\\) goes to 0. We could also discover that the empirical bias with small \\(r\\) performs better than large \\(r\\) in homogeneous subgroups setting. However, this situation is reversed in heterogeneous setting. Thus, the relationship between modification, heterogeneity, and number of subgroups might be a potential issue to work on. Another possible extension associates with the confounding adjustment, especially for trial data and observational studies. This method allows overlapped subgroup; that is, we cannot think the subgroup as the stratification. With confounding adjustment, the subgroup effect size should be reconsidered.","link":"/posts/issct/"},{"title":"Survival Analysis Note","text":"This lecture note is based on UTMDA/GSBS GS01 1023: Survival Analysis. All the lecture notes will provide an introduction to associated theory for the analysis of censored failure time data. Becuase this course mainly discuss the commonly-used methods with heuristic arguments and intuitive explanation of their properties, I will also include my own reading notes on incomplete observations and other types of continuous-time processes such as arise with competing risks data, multivariate failure time data, counting processes approaches, martingale and large-sample properties of estimators and tests, and some possible extensions. Chapter 1: Introduction File","link":"/posts/survivalnote/"},{"title":"å› æœæ¨è«–ç­†è¨˜ï¼ˆä¸€ï¼‰ï¼šWhat is Causality?","text":"Intro æœ‰é‘’æ–¼éå»åœ¨ä¸­ç ”é™¢æ“”ä»»RAæ™‚ï¼Œä¸¦æ²’æœ‰å»ºç«‹è‰¯å¥½çš„ç¿’æ…£ä¾†è¨˜éŒ„è‡ªå·±ä¸‰å¹´ä¾†çš„æ‰€å­¸ï¼Œè€Œé€™å€‹ç¶²ç«™ç‚ºäº†åŸ¹é¤Šé€™å€‹ç¿’æ…£ä¹‹é¤˜ï¼Œæˆ‘ä¹Ÿå¸Œæœ›èƒ½ä¸€é»é»çš„æŠŠéå»æ‰€å­¸å»ºæª”ï¼Œä¸¦ä¸”åœ¨æœªä¾†èƒ½æœ‰æ‰€ç”¨ï¼Œå› æ­¤æ¥ä¸‹ä¾†æœƒæœ‰ä¸€ç³»åˆ—æˆ‘å€‹äººå°æ–¼å› æœæ¨è«–çš„æ‘˜è¦ï¼Œä»¥åŠå› æœä¸­ä»‹åˆ†æè«–æ–‡çš„å…§å®¹ä»‹ç´¹ã€‚ ç•¶ç„¶ï¼Œè¿‘ä¾†å› æœæ¨è«–åœ¨AIçš„é ˜åŸŸç›¸ç•¶ç››è¡Œï¼Œè€Œå°é€™é ˜åŸŸæœ‰äº›äº†è§£çš„äººå¿…ç„¶çŸ¥é“ï¼Œäº‹å¯¦ä¸Šåœ¨40å¹´å‰å°±å·²ç¶“é–‹å§‹æœ‰é›»è…¦ç§‘å­¸å®¶åœ¨é€²è¡Œå› æœæ¨è«–çš„ç ”ç©¶ï¼Œè€Œä»–å€‘æ˜¯ä½¿ç”¨ä¸€ç¨®ç¨±ç‚ºDirected Acyclic Graphs (DAG) çš„æ¨¡å¼ä¾†ç ”ç©¶AIï¼Œè€Œå…¶ä¸­æœ€è‘—åçš„äººå°±å±¬åœ¨UCLAè³‡å·¥ç³»ä»»æ•™çš„Dr. Judea Pearlã€‚ä½†åœ¨æµè¡Œç—…å­¸èˆ‡çµ±è¨ˆå­¸é ˜åŸŸä¸­ä¹Ÿæœ‰å…©ä½ç›¸ç•¶è‘—åçš„æ•™æˆï¼Œåˆ†åˆ¥æ˜¯Dr. James Robins ä»¥åŠDr. Don Rubinå…©ä½åœ¨Harvardçš„è€å¸«ï¼ˆç”šè‡³å…©ä½è€å¸«çš„è§€é»ä¹Ÿæœ‰äº›ä¸åŒï¼‰ï¼Œè€Œæ¥ä¸‹ä¾†çš„ä»‹ç´¹æœƒæ›´æ¥è¿‘æµè¡Œç—…å­¸èˆ‡çµ±è¨ˆå­¸ä¸­ä½¿ç”¨çš„å› æœæ¨è«–ç‚ºä¸»ï¼Œå› æ­¤å…§å®¹æœƒåŒ…å«è¼ƒå¤šçš„ç¬¦è™Ÿèˆ‡çµ±è¨ˆæ¦‚å¿µï¼Œå¦‚æœå°è³‡å·¥ä»¥åŠAIæœ‰èˆˆè¶£çš„äººå¯ä»¥å»é–±è®€Dr. Judea Pearlæ‰€å¯«çš„â€œCausalityâ€ï¼Œæˆ–è€…æ˜¯æ›´ç‚ºé€šä¿—çš„â€œå› æœé©å‘½â€ã€‚ Overview åœ¨å› æœæ¨è«–ä¸­ï¼Œäº†è§£å› æœä¹‹å‰å¿…é ˆè¦å…ˆäº†è§£å› æœé—œä¿‚ã€ç›¸é—œã€ä»¥åŠäº¤äº’ä½œç”¨çš„å·®ç•°ï¼Œäº†è§£é€™äº›å°±èƒ½å°ä¸ç›´è§€çš„çŸ›ç›¾æ‰¾å‡ºæ›´åˆç†çš„è§£é‡‹ï¼Œå¦‚Simpson Paradoxã€‚ç•¶æˆ‘å€‘æœ‰äº†æ¸…æ¥šçš„åˆ†ç•Œï¼Œæ¥ä¸‹ä¾†å°±èƒ½å°‡å› æœæ¨è«–æ“´å±•åˆ°æœ€å¸¸ç”¨çš„å…©å€‹ä¸»é¡Œï¼šéš¨æ©Ÿè©¦é©—ä»¥åŠè§€å¯Ÿæ€§ç ”ç©¶ï¼Œä¸¦ä¸”ä»‹ç´¹å› æœæ¨è«–åœ¨é€™å…©å€‹ä¸»é¡Œä¸Šçš„å„ç¨®æ‡‰ç”¨ï¼ŒåŒ…å«å‰è¿°çš„DAGï¼Œä»¥åŠç¶“å¸¸å‡ºç¾çš„é¸æ“‡åèª¤è·Ÿå¹²æ“¾å› å­åœ¨å› æœæ¨è«–ä¸­çš„è™•ç†ç­‰ç­‰ã€‚ç•Œå®šå¥½ä¸Šè¿°å•é¡Œå¾Œï¼Œå¯ä»¥å°‡å› æœæ¨è«–æ“´å±•åˆ°å› æœä¸­ä»‹åˆ†æï¼Œä»¥åŠåœ¨ç¶“æ¿Ÿå­¸ã€æ”¿æ²»å­¸ä»¥åŠéºå‚³å­¸ä¸­å¸¸è¦‹çš„å·¥å…·è®Šé‡ (Instrumental Variableï¼Œåœ¨éºå‚³å­¸ä¸­å‰‡ç‚ºMendelian Randomization)ï¼Œæœ€å¾Œå‰‡æœƒåŸºæ–¼å› æœä¸­ä»‹åˆ†æä¾†ä»‹ç´¹ä¸€äº›é–±è®€éæˆ–è€…æ˜¯æˆ‘æœ‰åƒèˆ‡çš„è«–æ–‡ã€‚ What is Causality? äººç”Ÿè€Œç‚ºäººæ°¸é éƒ½åœ¨å°‹æ‰¾å› èˆ‡æœï¼Œä¸ç®¡æ˜¯ä¿—è«ºä¸­çš„ã€Œç¨®ä»€éº¼å› ï¼Œå¾—ä»€éº¼æœã€ã€ä½›æ•™ä¿¡ä»°ä¸­çš„ã€Œå› æœè¼ªè¿´ã€ã€ä¹ƒè‡³æ–¼ä»Šå¤©å­¸ç¿’ç§‘å­¸ä¸­çš„ã€Œå› æœé—œä¿‚ã€ï¼Œå†å†è­‰æ˜äººé¡å°å› æœæ¨è«–çš„æœ¬èƒ½èˆ‡æ¸´æ±‚ï¼Œå¯å¤§å¤šæ•¸æ™‚å€™æˆ‘å€‘ä¸æ±‚ç”šè§£ï¼Œç¶“å¸¸æ€§åœ°å°‡åƒ…åƒ…æ˜¯çœ¼ç›çœ‹è¦‹çš„è³‡è¨Šï¼ˆäº¦å³çµ±è¨ˆä¸Šçš„observationï¼‰å¾—åˆ°çš„é—œä¿‚ä½œç‚ºå› æœï¼Œé€ æˆäº†çµ±è¨ˆä¸Šçš„åè¨€ï¼šã€Œç›¸é—œä¸¦ä¸ä»£è¡¨å› æœã€ï¼Œç„¶è€Œæˆ‘å€‘å»ç¼ºä¹é©ç•¶çš„æ•¸å­¸å·¥å…·åœ¨çµ±è¨ˆä¸­æè¿°å› æœä¸¦è§£é‡‹ä¹‹ã€‚é›–ç„¶æˆ‘å€‘å¯ä»¥é€éæ—¥å¸¸çš„äº‹ä»¶ä¾†æ€è€ƒç”šè‡³å›ç­”å…©ä»¶äº‹æƒ…æ˜¯å¦ç‚ºå› æœï¼Œä¾‹å¦‚ï¼šç–¾ç—…æ²»ç™‚çš„æ•ˆæœã€å†°æ·‡æ·‹éŠ·é‡èˆ‡é¯Šé­šæ”»æ“Šæ•¸é‡çš„æ­£ç›¸é—œã€éŠ·å”®é¡çš„å¢åŠ è¦æ­¸å› æ–¼å»£å‘Šé‚„æ˜¯åƒ¹æ ¼ç­‰ï¼Œäº‹å¯¦ä¸Šæƒ³è¦åœ¨æ•¸å­¸ä¸Šå»ºç«‹å› æœäº‹å¯¦ä¸Šéœ€è¦æ›´å¤šçš„æ¢ä»¶ï¼Œå¦‚è—¥å» åœ¨é€²è¡Œè—¥ç‰©é–‹ç™¼èˆ‡ç™‚æ•ˆè©•ä¼°æ™‚ï¼Œå¿…é ˆè¦å‰µé€ å‡ºéš¨æ©Ÿæ§åˆ¶è©¦é©— (Randomized Control Trial) é€™æ¨£è¿‘ä¹å®Œç¾çš„è©¦é©—ï¼Œç”šè‡³é‚„å¾—è¤‡è£½æ•¸æ¬¡æ‰å¾—ä»¥æ‰¾å‡ºå› æœé—œä¿‚ï¼Œè€Œé€™ä»¶äº‹æ°æ°èªªæ˜å› æœæ¨è«–çš„æ ¸å¿ƒè§€å¿µï¼šæ§åˆ¶æ‰€æœ‰è®Šå› ä¸‹çš„å¯è¤‡è£½çµæœ æƒ³è¦å‰µé€ å‡ºé€™æ¨£å®Œç¾çš„å¯¦é©—ï¼Œæˆ‘å€‘å¿…é ˆè¦å…ˆæœ‰ã€Œåäº‹å¯¦æ¶æ§‹ (Counterfactual framework)ã€ ä»¥åŠã€Œä»‹å…¥ (Intervention)ã€ï¼Œcounterfactualä»¥ä¸­æ–‡å¸¸è¦‹çš„èªå½™ä¾†èªªæ˜¯ã€Œå¦‚æœåšäº†ä»€éº¼ã€ä»‹å…¥ã€ï¼Œæœƒæ€æ¨£ï¼Ÿã€ä¹Ÿå°±æ˜¯æˆ‘å€‘å‰µé€ äº†å¦å¤–ä¸€å€‹å¹³è¡Œä¸–ç•Œä¾†æ¨è«–ï¼Œç„¶å¾Œé€éinterventionä¾†æ¨è«–å› æœæ˜¯å¦å­˜åœ¨ã€‚ä¾‹å¦‚ï¼šé›–ç„¶æˆ‘å€‘éƒ½èªªé›å•¼ä¹‹å¾Œå¤ªé™½å°±æœƒå‡èµ·ï¼Œä½†æˆ‘å€‘å¯ä»¥ç›´è¦ºå¾—çŸ¥åœ¨å¦å¤–ä¸€å€‹å¹³è¡Œä¸–ç•Œä¸­ï¼Œå³ä½¿ä»Šå¤©æŠŠé›ç‡‰æˆé›æ¹¯ï¼Œå¤ªé™½ä¾ç„¶ç…§å¸¸å‡èµ·ã€‚æˆ‘å€‘ä¹Ÿèƒ½å¾é€™æ¨£çš„ä¾‹å­çŸ¥é“ï¼Œå› æœé—œä¿‚åŒæ™‚ä¹Ÿæ½›è—æ™‚é–“çš„å…ˆå¾Œé †åºæ€§ï¼Œä»¥åŠå› -&gt;æœé€™æ¨£çš„ä¸€å€‹ç®­é ­æ–¹å‘ã€‚ç‚ºä»€éº¼ç”Ÿç‰©çµ±è¨ˆã€é†«å­¸ã€æµè¡Œç—…å­¸ã€è©¦é©—ç­‰ç­‰æ–¹æ³•è«–ç‰½æ¶‰æ›´å¤šçš„å› æœæ¨è«–ï¼Ÿå› ç‚ºäººåªæœ‰ä¸€æ¢ç”Ÿå‘½èˆ‡ä¸€å€‹æ­·ç¨‹ï¼Œæˆ‘å€‘æ²’æœ‰è¾¦æ³•åœ¨ç¾å¯¦çš„è§€å¯Ÿä¸­å¾—åˆ°å¦å¤–ä¸€å€‹ä¸€æ¨¡ä¸€æ¨£çš„ä½ è·Ÿæˆ‘ï¼ŒåŠ ä¸Šåœ¨é†«å­¸çš„ç ”ç©¶ä¸­ä¸èƒ½å‡ºå·®éŒ¯ï¼Œç•¢ç«Ÿäººæ­»ä¸èƒ½å¾©ç”Ÿï¼Œæ‰€ä»¥åœ¨é€™æ–¹é¢å°±éœ€è¦æ›´åš´è¬¹çš„å› æœæ¨è«–æ¶æ§‹ï¼Œä¸åƒåšæ©Ÿå™¨å­¸ç¿’å¯ä»¥æ›´æœ‰å½ˆæ€§çš„èª¿æ•´åƒæ•¸è€Œä¸å°çœŸå¯¦äººå‘½æœ‰å½±éŸ¿ï¼ˆä¹Ÿä¸æœƒè¢«å‘Šï¼‰ï¼Œäº‹å¯¦ä¸Šä»»ä½•çš„é æ¸¬æ–¹æ³•å…¶å¯¦éƒ½æœªå¿…éœ€è¦å› æœæ¨è«–ï¼Œå¤§éƒ¨åˆ†ç¾å­˜çš„æ©Ÿå™¨å­¸ç¿’æ–¹æ³•ä¹Ÿå¾ˆé›£è¾¯è­‰å¹²æ“¾å› å­ä»¥åŠåèª¤ã€‚ ç•¶æˆ‘å€‘çŸ¥é“äº†å› æœæ¨è«–çš„é‡è¦æ€§ä¹‹å¾Œï¼Œä¸‹ä¸€æ­¥å‰‡è¨è«–å¦‚ä½•çµåˆå› æœæ¨è«–èˆ‡çµ±è¨ˆæ¨è«–ã€‚é¦–å…ˆå›åˆ°çµ±è¨ˆçš„è§€å¿µï¼Œé€²è¡Œæ¨è«–ä¹‹å‰æˆ‘å€‘å¿…é ˆè¦æœ‰ç›®æ¨™çš„estimandï¼Œé€™å€‹å¾…ä¼°è¨ˆçš„é‡å¯ä»¥è®“æˆ‘å€‘ç”¨ä¼°è¨ˆé‡ (estimator) ä¾†è¿‘ä¼¼æˆ‘å€‘æƒ³å¾è³‡æ–™ä¸­æ¨æ–·å‡ºçš„æ¯é«”è³‡è¨Šï¼Œä½†åœ¨å‰µé€ estimandæ™‚ï¼Œæ¯”èµ·ä½¿ç”¨æ©Ÿç‡èˆ‡æ¨¡å‹ï¼Œæˆ‘å€‘æ›´å¤šæ™‚å€™æ˜¯å„ªå…ˆè€ƒæ…®æ‰¾åˆ°ä¸€å€‹æƒ³å¾—åˆ°çš„è³‡è¨Šçš„è¡¨ç¾å½¢å¼ï¼ˆå¦‚æˆ‘å€‘æƒ³è¡¡é‡ç™‚æ³•èƒ½é™ä½å¤šå°‘æ­»äº¡é¢¨éšªï¼‰ï¼Œå†å»ä½¿ç”¨æ¨¡å‹ä¾†æƒ³è¾¦æ³•å¾—åˆ°estimatoré€™å€‹è¡¨ç¾å½¢å¼ï¼ˆå¦‚log hazard ratioï¼‰ï¼Œæ‰å¾—åˆ°ä¼°è¨ˆå€¼ (estimate) é€™å€‹è¿‘ä¼¼å€¼ï¼Œæœ€å¾Œå°±æ˜¯æŠŠå› æœçš„ç›´è§€èˆ‡çµ±è¨ˆæ¨è«–é€£æ¥ï¼Œä¸¦ä¸”åœ¨æœ‰å…±è­˜çš„åäº‹å¯¦æ¡†æ¶ä¸‹æœ‰æ•ˆåœ°é€éå› æœä¾†æºé€šã€‚å› æ­¤ï¼Œåœ¨å®šç¾©å•é¡Œèˆ‡é—¡é‡‹å› æœçš„éšæ®µï¼Œæˆ‘å€‘å¿…é ˆåœ¨è€ƒæ…®è³‡æ–™ç‰¹æ€§å‰å°±å…ˆè©•ä¼°å› æœï¼Œè€Œä¸æ˜¯å„ªå…ˆè€ƒæ…®å¯ç”¨æ¨¡å‹ï¼Œä¸¦ä¸”é©ç•¶çš„ä½¿ç”¨ä¸€äº›ç„¡é ˆè€ƒæ…®è³‡æ–™ç‰¹æ€§çš„å› æœå‡è¨­ä¾†å‰µé€ ä¸€å€‹å‰è¿°çš„å¹³è¡Œä¸–ç•Œï¼Œé€™äº›å‡è¨­æˆ‘å…¶å¯¦å¾ˆå¸¸ç¨±å‘¼ç‚ºå¤šæ‹‰Aå¤¢çš„æ™‚å…‰æ©Ÿï¼Œè€Œé€™äº›å‡è¨­æœƒåœ¨å¾Œé¢æåˆ°ã€‚ Conclusion ç¶œä¸Šæ‰€è¿°ï¼Œæˆ‘å€‘å¿…é ˆäº†è§£åˆ°å› æœæ¨è«–æ˜¯ç™¼ç”Ÿåœ¨çµ±è¨ˆæ¨è«–ä¹‹å‰ï¼Œä¸¦ä¸”æˆ‘å€‘ä¸æœƒå› ç‚ºæœ‰äº†Causal inferenceè®“æˆ‘å€‘çš„çµ±è¨ˆæ¨è«–å¾—åˆ°å¤©ç¿»åœ°è¦†çš„è®ŠåŒ–æˆ–æ˜¯ç™¼å±•å‡ºå…¨æ–°çš„çµ±è¨ˆé‡æˆç‚ºcausalä¸€æ–¹ä¹‹éœ¸ï¼Œä¹Ÿä¸æ˜¯å› ç‚ºç†è§£causalè€Œé–‹å˜´ä»»ä½•çš„çµ±è¨ˆæ–¹æ³•éƒ½æ²’æœ‰å› æœé—œä¿‚æˆ–ä¸ç¬¦åˆå‡è¨­ï¼Œç•¢ç«Ÿæˆ‘å€‘å¦‚æœæŠŠç ”ç©¶æ”¾åˆ°é‡å­ç³¾çºçš„å±¤æ¬¡ï¼Œæ°¸é éƒ½æ²’è¾¦æ³•æ’é™¤æ‰é‚£éš±å«çš„ä¸ç¢ºå®šæ€§ã€‚å› æœæ¨è«–çš„ç§‘å­¸å•é¡Œæ›´è‘—é‡åœ¨ç¢ºä¿æ€æ¨£çš„æ¢ä»¶èˆ‡å‡è¨­ä¸‹ï¼Œä½¿å¾—å› æœæˆç«‹ï¼Œä¸¦ä¸”å¾—åˆ°æˆ‘å€‘æœ‰èˆˆè¶£ä¸”å…·æœ‰å› æœé—œä¿‚çš„æ¨™çš„ï¼Œæˆ‘æƒ³é€™æ‰æ˜¯å› æœæ¨è«–çš„ç²¾é«“ã€‚ (To be continued)","link":"/posts/causal1/"},{"title":"å› æœæ¨è«–ç­†è¨˜ï¼ˆäºŒï¼‰ï¼šCausal Effect","text":"Intro ä¸Šä¸€ç¯‡ç­†è¨˜ä¸­ç°¡å–®æ•˜è¿°å› æœæ¨è«–çš„é‡è¦æ€§ï¼Œä»¥åŠèˆ‡çµ±è¨ˆæ¨è«–çµåˆå¦‚ä½•å¯¦ç¾ï¼Œé€™ä»½ç­†è¨˜å°‡æœƒå¾æ•¸å­¸ç¬¦è™Ÿèµ·æ‰‹ï¼Œä¾†ä»‹ç´¹counterfactualçš„æ¶æ§‹ï¼Œä¸¦ä¸”å¾—åˆ°causal effectã€‚åœ¨é–‹å§‹ä¹‹å‰å…ˆèˆ‰å€‹å¯¦éš›ä¸Šçš„ä¾‹å­ï¼šæˆ‘å€‘æƒ³è¦è©•ä¼°ç‚æ€§ä¹³è…ºç™Œ (Inflammatory breast cancer)çš„ç—…æ‚£ä¸­ï¼Œå¦‚æœæ–½è¡Œä¸‰åˆä¸€ç™‚æ³• (\\(S\\))å¾Œï¼Œå­˜æ´»èˆ‡å¦ (\\(Y\\)) çš„å·®åˆ¥ã€‚ä¸‹é¢å°‡æœƒä»¥é€™å€‹èˆ‰ä¾‹è²«ç©¿å…¨æ–‡ã€‚ Counterfactual notation and Average causal effect å…ˆå›åˆ°çµ±è¨ˆä¸Šçš„ç¬¦è™Ÿï¼Œåœ¨çµ±è¨ˆä¸­é€šå¸¸å¤§å¯«ä»£è¡¨äº†éš¨æ©Ÿè®Šæ•¸ (random variables)ï¼Œå› æ­¤æˆ‘å€‘åœ¨å»ºæ§‹é€™äº›ç¬¦è™Ÿæ™‚ï¼Œä¹Ÿæ˜¯ä»¥éš¨æ©Ÿè®Šæ•¸çš„ç²¾ç¥ä¾†æ€è€ƒã€‚é¦–å…ˆæˆ‘å€‘æœƒæœ‰å…©å€‹å¹³è¡Œå®‡å®™ï¼Œåœ¨ç¬¬ä¸€å€‹å®‡å®™ä¸­æŸç—…äººæ–½è¡Œäº†ä¸‰åˆä¸€ç™‚æ³•ï¼Œè¢«æˆ‘å€‘æ¨™è¨˜ç‚º\\(Y_i(s=1)\\)ï¼Œå³ä½¿æ˜¯åœ¨é€™å€‹æ–½è¡Œä¸‰åˆä¸€ç™‚æ³•çš„å®‡å®™ä¸­ï¼Œç—…äººæ˜¯å¦å­˜æ´»ä»ç„¶å­˜åœ¨è‘—ä¸ç¢ºå®šæ€§ï¼Œäº¦å³\\(Y_i(s=1)\\)ä¹Ÿæ˜¯ä¸€å€‹éš¨æ©Ÿè®Šæ•¸ï¼Œæˆ‘å€‘æœƒä»¥ä¸€å€‹æ©Ÿç‡æ¨¡å‹æè¿°ä»–ï¼›åŒç†ï¼Œåœ¨å¦ä¸€å€‹å¹³è¡Œå®‡å®™ä¸­è©²ç—…äººä¸¦æœªæ–½è¡Œä¸‰åˆä¸€ç™‚æ³•ï¼Œæˆ‘å€‘å°‡å…¶æ¨™ç‚ºå¦ä¸€å€‹éš¨æ©Ÿè®Šæ•¸\\(Y_i(s=0)\\)ï¼Œè‹¥\\(Y_i(s=1)-Y_i(s=0)\\neq 0\\)ï¼Œå³æ˜¯æœ‰individual causal effectï¼Œè€Œé€™è£¡çš„\\(Y(s)\\)ï¼Œé€šå¸¸ç¨±ä¹‹ç‚ºcounterfactual outcomeæˆ–æ˜¯potential outcomeã€‚ç„¶è€Œï¼Œæˆ‘å€‘åƒ…æœ‰æ¥å—æ²»ç™‚æˆ–æœªæ¥å—æ²»ç™‚ä¸­çš„ä¸€å€‹æƒ…æ³ç™¼ç”Ÿï¼Œè‹¥æŸç—…äººå¯¦éš›ä¸Šæ¥å—æ²»ç™‚ (\\(S\\)=1)ï¼Œå‰‡counterfactual outcomeä¸­ \\(Y_i(s=1)=Y_i|(S=1)\\)ï¼ˆe.g. å…·æœ‰ç›¸åŒçš„p.d.f.ï¼‰ï¼Œä¹Ÿå°±æ˜¯æˆ‘å€‘è§€å¯Ÿåˆ°çš„outcomeç­‰æ–¼counterfactual outcomeï¼Œé€™æ€§è³ªæˆ‘å€‘ç¨±ä¹‹ç‚ºcausal consistencyï¼Œä»¥ç™½è©±æ–‡ä¾†æ•˜è¿°å°±æ˜¯å¦‚æœæŸç—…äººåœ¨ç‰©ç†ä¸–ç•Œä¸­æ¥å—æŸå€‹æ²»ç™‚ï¼Œå¹³è¡Œå®‡å®™ä¸­åŒæ¨£æ¥å—æ²»ç™‚çš„è©²ç—…äººæœƒå…·æœ‰ç›¸åŒçš„æ­·ç¨‹ã€‚ ç”±æ–¼ä¸å¯èƒ½åœ¨ç¾å¯¦ä¸­æ‰¾åˆ°åŒä¸€å€‹äººåŒæ™‚æ¥å—èˆ‡ä¸æ¥å—æ²»ç™‚ï¼Œå› æ­¤\\(Y_i(s=1)\\)èˆ‡\\(Y_i(s=0)\\)å¿…ç„¶æœ‰ä¸€å€‹æ˜¯è§€å¯Ÿä¸åˆ°(missing dataï¼ŒåŒæ¨£çš„å› æœæ¨è«–ä¹Ÿæ˜¯åœ¨è™•ç†missiing dataçš„å•é¡Œï¼) ï¼Œé‚£å°±ç„¡æ³•å¾—åˆ°é€™å€‹ç—…äººçš„individual causal effectï¼Œæ­¤è™•æˆ‘å€‘éœ€è¦Average causal effectä¾†å”åŠ©æˆ‘å€‘æ‰¾åˆ°ç¾¤é«”æ²»ç™‚èˆ‡å¦çš„å½±éŸ¿ï¼Œä¹Ÿå°±é–‹å§‹æœƒæœ‰æ©Ÿç‡èˆ‡çµ±è¨ˆçš„ä»‹å…¥ï¼Œæ­¤è™•é™¤äº†ä¸€æ¨£éœ€è¦randomizationä¾†ç¢ºä¿å…©å€‹ä¸åŒæ²»ç™‚ä¹‹ä¸‹çš„ç—…äººç¾¤é«”æ˜¯é¡ä¼¼çš„ä¹‹å¤–ï¼Œä¹Ÿéœ€è¦é€™å€‹ç¾¤é«”è¶³å¤ å¤§ä»¥æ»¿è¶³å¤§æ¨£æœ¬æ€§è³ªä¾†å¹«åŠ©æˆ‘å€‘æ‰¾åˆ°é¡ä¼¼çš„äººã€‚Average causal effectçš„å®šç¾©ç›¸ç•¶ç°¡å–®ï¼Œç”±æ–¼æˆ‘å€‘æ¨è«–çš„æ˜¯ç¾¤é«”è³‡è¨Šï¼Œå› æ­¤average causal effectçš„é—œä¿‚å¼ç‚º\\(P[Y(s=1)]\\neq P[Y(s=0)]\\)ï¼Œåˆæˆ–è€…æ˜¯\\(E[Y(s=1)]\\neq E[Y(s=0)]\\)ï¼Œæ›´å»£ç¾©çš„ä¾†èªªï¼Œæˆ‘å€‘ä¹Ÿå¯ä»¥ç”¨ä¸­ä½æ•¸æˆ–è€…æ˜¯empirical distribbutionä¾†å®šç¾©average causal effectï¼Œåªè¦causal effectæ˜¯å…©å€‹ä¸åŒcounterfactual outcomeå„è‡ªçš„marginal distributionçš„å‡½æ•¸ç›¸äº’æ¯”è¼ƒå³å¯ã€‚ Causal effect measure and Association measure æœ‰äº†å‰è¿°çš„é—œä¿‚å¼ï¼Œæˆ‘å€‘å¾—ä»¥å‰µé€ å› æœæ¨è«–ä¸Šçš„å‡èªªæª¢å®š\\(H_0: P[Y(s=1)=1]= P[Y(s=0)=1]\\)ï¼Œæ–¹ä¾¿æˆ‘å€‘é‡æ¸¬average causal effectçš„estimandæœ‰ä»¥ä¸‹å¹¾ç¨®ï¼š \\[\\begin{cases} \\text{Causal risk difference:} &amp;P[Y(s=1)=1]- P[Y(s=0)=1]=0\\\\ \\text{Causal risk ratio:} &amp;\\dfrac{P[Y(s=1)=1]}{P[Y(s=0)=1]}=1\\\\ \\text{Causal odds ratio:} &amp;\\dfrac{P[Y(s=1)=1]/P[Y(s=1)=0]}{P[Y(s=0)=1]/P[Y(s=0)=0]}=1\\\\ \\text{Number needed to treat:} &amp;NNT=\\dfrac{-1}{P[Y(s=1)=1]- P[Y(s=0)=1]} \\end{cases}\\] ç„¶è€Œï¼Œé€™è·Ÿå¹³å¸¸æ‰€å­¸çš„ä¼¼ä¹ä¸å¤ªä¸€æ¨£ï¼Ÿè€ƒæ…®associationçš„ä¸Šè¿°estimandså¯ä»¥è¡¨ç¤ºç‚ºï¼š \\[\\begin{cases} \\text{risk difference:} &amp;P[Y=1|S=1]- P[Y=1|S=0]=0\\\\ \\text{risk ratio:} &amp;\\dfrac{P[Y=1|S=1]}{P[Y=1|S=0]}=1\\\\ \\text{odds ratio:} &amp;\\dfrac{P[Y=1|S=1]/P[Y=0|S=1]}{P[Y=1|S=0]/P[Y=0|S=0]}=1 \\end{cases}\\] ä»¥ä¸‹å€Ÿç”¨éå»çš„æŠ•å½±ç‰‡ä¾†æè¿°é€™å…©è€…æœ‰ä½•ä¸åŒä¹‹è™• ç‰©ç†ä¸–ç•Œä¸­æˆ‘å€‘èƒ½è§€å¯Ÿåˆ°çš„è³‡æ–™å°±æ˜¯å·¦é‚Šçš„æ–¹å½¢ï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ°å¦‚æœæ˜¯ç›¸é—œæ€§ï¼Œæˆ‘å€‘æ˜¯åœ¨æ¯”è¼ƒè§€å¯Ÿåˆ°ä¸åŒ\\(S\\)æ¢ä»¶ä¸‹å€‹åˆ¥çš„\\(Y\\)ï¼Œè€Œå› æœå‰‡æ˜¯å°‡ç›¸é—œæ€§æ¯”è¼ƒä¸­ç¼ºæ¼çš„éƒ¨åˆ†è£œæˆå®Œæ•´çš„æ–¹å½¢ï¼Œä¹Ÿå°±æ˜¯åœ¨æ¯”è¼ƒå¦‚æœæ•´å€‹ç¾¤é«”åœ¨ä¸åŒ\\(S\\)æ¢ä»¶ä¸‹çš„\\(Y\\) å› æ­¤è‹¥æ˜¯è§€å¯Ÿåˆ°çš„è³‡æ–™æœƒå› ç‚º\\(S\\)çš„é—œä¿‚è€Œç”¢ç”Ÿä¸ä¸€æ¨£çš„åˆ†é…ï¼ˆå¦‚æ–¹å½¢ä¸­çš„è—é»ï¼‰ï¼Œå¾ˆé¡¯ç„¶èƒ½ç™¼ç¾ç›¸é—œæ€§åœ¨æ­¤åˆ»ä¸ç­‰æ–¼å› æœ è€Œ\\(S\\)è‹¥å°æ–¼è—é»çš„åˆ†é…ä¸¦ç„¡å½±éŸ¿ï¼Œä¹Ÿå°±æ˜¯å¦‚æœæ²’æœ‰å¹²æ“¾å› å­ï¼Œå‰‡ç›¸é—œæ€§å°±èƒ½ç­‰æ–¼å› æœï¼ (To be continued)","link":"/posts/causal2/"},{"title":"Implementations of the Monte Carlo EM Algorithm","text":"This note is based on the R. Levine and G. Casella (20), Implementations of the Monte Carlo EM Algorithm, Journal of Computational and Graphical Statistics Monte Carlo EM Algorithm Overview MCEM is a modification of the EM algorithm where the conditional expectation of log-likelihood in the E-step is computed numerically through Monte Carlo simulations, and MCMC sampler (Gibbs Sampler / Metropolis-Hasting) is the most flexible approach for Monte Carlo sample. Target Minimizing computational cost from non-closed form E-step (Problem from Wei(1990) original paper) to approximate the posterior: importance sampling whereby samples drawn during previous EM iterations are recycled Choosing sample size: regenerate approximate samples by subsampling the generated MCMC sample during different renewal periods. Existing paper Booth an Hobert(1990): Gauge error by increase sample size as algorithm converges, but requires i.i.d. samples or importance weighted. MCMC: Though the random variates are dependent in such a scenario, the E-step estimator is still unbiased and approaches the true value as the sample size increases. Notation Observed data: \\(\\mathbf{y} = (y_1,\\cdots,y_n)^T\\) Parameters: \\(\\mathbf{\\Psi}\\) MLE is simpler to compute on the data augmented by a set of latent variables \\(\\mathbf{u}=(u_1,\\cdots,u_q)^T\\) conditional distribution of the latent variables: \\(g\\left(\\mathbf{u}|\\mathbf{y,\\Psi^{(r)}}\\right)\\) with iteration \\(r\\) EM: Complete data likelihood: \\(Q\\left(\\mathbf{\\Psi}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)=E_{\\hat{\\mathbf{\\Psi}}^{(r)}}\\{\\text{ln} f(\\mathbf{y,u}|\\mathbf{\\Psi})|\\mathbf{y}\\}=\\int\\text{ln} f(\\mathbf{y,u}|\\mathbf{\\Psi})g\\left(\\mathbf{u}|\\mathbf{y,\\Psi^{(r)}}\\right)d\\mathbf{u}\\) and maximize it over \\(\\mathbf{\\Psi}\\) Obtain a sample \\(\\mathbf{u^{(r)}_1},\\cdots,\\mathbf{u^{(r)}_m}\\) from conditional distribution, the expectation can be estimated by Monte Carlo sum \\(Q\\left(\\mathbf{\\Psi}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)=\\dfrac{1}{m}\\sum\\limits^m_{t=1}\\text{ln}f\\left(\\mathbf{y,u^{(r)}_t|\\Psi}\\right)\\) \\(m\\) denotes the dependence of this estimator on the MC sample size. So, how could we obtain a random sample from \\(g\\left(\\mathbf{u}|\\mathbf{y,\\Psi}\\right)\\) , and how do we choose \\(m\\)? Importance Sampling Drawing an MCMC sample each iteration of the EM algorithm could be prohibitively costly particularly for large \\(m\\). Importance weight \\(w_t=\\dfrac{g\\left(\\mathbf{u}_t|\\mathbf{y,\\Psi^{(r)}}\\right)}{g\\left(\\mathbf{u}_t|\\mathbf{y,\\Psi^{(0)}}\\right)}=\\dfrac{L(\\mathbf{\\hat{\\Psi}^{(r)}|u_t,y}))/L(\\mathbf{\\hat{\\Psi}^{(r)}|y})}{L(\\mathbf{\\hat{\\Psi}^{(0)}|u_t,y})/L(\\mathbf{\\hat{\\Psi}^{(0)}|y})}\\) corrects the original sample \\(\\mathbf{u}\\) with the new information we have at iteration \\(r\\) How much expense do we save? The expense saving with respect to the weights are not dependent on the unknown likelihood \\(L(\\mathbf{\\Psi|y})\\) because it would be cancelled Reasonable choice: \\(Q_m\\left(\\mathbf{\\Psi}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)=\\dfrac{\\sum\\limits^m_{t=1}w_t\\text{ln}f\\left(\\mathbf{y,u^{(r)}_t|\\Psi}\\right)}{\\sum\\limits^m_{i=1}w_t}=\\dfrac{\\sum\\limits^m_{t=1}w'_t\\text{ln}f\\left(\\mathbf{y,u_t|\\Psi}\\right)}{\\sum\\limits^m_{i=1}w'_t}\\) \\(w'_t=\\dfrac{L(\\mathbf{\\hat{\\Psi}^{(r)}|u_t,y})}{L(\\mathbf{\\Psi^{(0)}|u_t,y})}\\) The reasonable choice has smaller MSE than the below one (Liu 1996 and Casella 1998). Another choice of important sampling: \\(Q_m\\left(\\mathbf{\\Psi}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)=\\dfrac{1}{m}\\sum\\limits^m_{t=1}w_t\\text{ln}f\\left(\\mathbf{y,u_t|\\Psi}\\right)\\). This would not affect the EM because the normalizing constant \\(\\dfrac{L(\\mathbf{\\Psi^{(0)}|y})}{L(\\mathbf{\\hat{\\Psi}^{(r)}|y})}\\) only depends on initial value and updated estimate with iteration \\(r\\) and does not involve the maximization. Use the reasonable one could avoid calculation of normalizing constant when choosing m. Burn-in in first few iteration: This approach fails when the estimates with \\(r\\) iteration is too far from the initial value. The target density would be close enough to have small variance. Burn-in time is problem specific depending on how close we need to be to the MLE in order to obtain stable importance weights in subsequent M-steps. Trial runs of the EM algorithm with importance sampling, gauging the variability in the importance weights as the algorithm converges, will provide the user with an idea for the appropriate burn-in time. Algorithm Initialize \\(m\\) and \\(\\mathbf{\\Psi}^{(0)}\\) Generate \\(\\mathbf{u_1},\\cdots,\\mathbf{u_m}\\sim g\\left(\\mathbf{u}|\\mathbf{y,\\Psi^{(0)}}\\right)\\) via MCMC At iteration \\(r+1\\), compute the importance weight \\(w_t=\\dfrac{L(\\mathbf{\\hat{\\Psi}^{(r)}|u_t,y})}{L(\\mathbf{\\Psi^{(0)}|u_t,y})}\\) E-step: estimate \\(Q\\left(\\mathbf{\\Psi}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)\\) by \\(Q_m\\left(\\mathbf{\\Psi}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)=\\dfrac{\\sum\\limits^m_{t=1}w_t\\text{ln}f\\left(\\mathbf{y,u_t|\\Psi}\\right)}{\\sum\\limits^m_{i=1}w_t}\\) M-step: \\(\\argmax\\limits_{\\Psi}Q_m\\left(\\mathbf{\\Psi}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)\\) to obtain \\(\\hat{\\mathbf{\\Psi}}^{(r+1)}\\) MC error estimation: Compute for each \\(j=1,\\cdots,s\\), \\(\\hat\\mu_{m;j}=\\sum\\limits^m_{t=1}w_t\\dfrac{\\partial}{\\partial \\psi(j)}\\text{ln}f\\left(\\mathbf{y,u_t|\\Psi}\\right)/\\sum\\limits^m_{t=1}w_t\\) by plugging in \\(\\mathbf{\\Psi}=\\hat{\\mathbf{\\Psi}}^{(r+1)}\\) Compute for each \\(j=1,\\cdots,s\\), \\(\\hat{v}_{m;j}=\\sum\\limits^m_{t=1}w_t\\left[\\dfrac{\\partial}{\\partial \\psi(j)}\\text{ln}f\\left(\\mathbf{y,u_t|\\Psi}\\right)\\right]^2/\\left(\\sum\\limits^m_{t=1}w_t-\\hat\\mu_{m;j}^2\\right)\\) by plugging in \\(\\mathbf{\\Psi}=\\hat{\\mathbf{\\Psi}}^{(r+1)}\\) Obtain the \\((1-\\alpha)\\) confidence interval about \\(Q^{(1)}_j\\left(\\mathbf{\\Psi}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)\\) by the above mean and variance from standard normal distribution. Obtain subsampling instants \\(t_k=x_1+\\cdots+x_k\\) where \\(x_k-1\\sim Poi(\\nu_k), k=1,\\cdots,N\\) and \\(N=\\sup\\{n;t_n\\leq m\\}\\) If \\(Q^{(1)}_m\\left(\\hat{\\mathbf{\\Psi}}^{(r)}|\\mathbf{\\Psi}^{(r-1)}\\right)\\) lies in confidence interval, then Set \\(m_o=m\\) Set \\(m=m_o+gauss(m_o+c)\\) for some \\(c&gt;0\\) Obtain \\(\\mathbf{u_{m_o+1}},\\cdots,\\mathbf{u_m}\\sim g\\left(\\mathbf{u}|\\mathbf{y,\\Psi^{(0)}}\\right)\\) Compute for each \\(j=1,\\cdots,s\\), \\(Q^{(1)}_{m;j}\\left(\\hat{\\mathbf{\\Psi}}^{(r+1)}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)=\\sum\\limits^N_{k=1}w_{t_k}\\dfrac{\\partial}{\\partial \\psi(j)}\\text{ln}f\\left(\\mathbf{y,u_{t_k}|\\Psi}\\right)/\\sum\\limits^m_{t=1}w_{t_k}\\) by plugging in \\(\\mathbf{\\Psi}=\\hat{\\mathbf{\\Psi}}^{(r+1)}\\) Repeat step 3 to step 9 until converge Burn-in steps run burn-in for one minutes Set importance weights \\(w_t = 1, \\forall t=1,\\cdots,m\\) at iteration \\(b\\) Generate \\(\\mathbf{u_1},\\cdots,\\mathbf{u_m}\\sim g\\left(\\mathbf{u}|\\mathbf{y,\\Psi^{(b)}}\\right)\\) via MCMC Run E and M step above with \\(r=b\\) Repeat Steps 2 and 3 for \\(B\\) burn-in iterations. Reinitialize \\(\\mathbf{\\Psi}^{(0)}=\\mathbf{\\Psi}^{(B)}\\)","link":"/posts/imcem/"}],"tags":[{"name":"å¿ƒå¾—","slug":"å¿ƒå¾—","link":"/tags/%E5%BF%83%E5%BE%97/"},{"name":"Note","slug":"Note","link":"/tags/Note/"},{"name":"Encrypted","slug":"Encrypted","link":"/tags/Encrypted/"},{"name":"Lecture Note","slug":"Lecture-Note","link":"/tags/Lecture-Note/"},{"name":"Subgroup Analysis","slug":"Subgroup-Analysis","link":"/tags/Subgroup-Analysis/"},{"name":"Clinical Trial","slug":"Clinical-Trial","link":"/tags/Clinical-Trial/"},{"name":"Causal","slug":"Causal","link":"/tags/Causal/"},{"name":"EM Algorithm","slug":"EM-Algorithm","link":"/tags/EM-Algorithm/"},{"name":"Monte Carlo","slug":"Monte-Carlo","link":"/tags/Monte-Carlo/"},{"name":"DeMiX-NB","slug":"DeMiX-NB","link":"/tags/DeMiX-NB/"}],"categories":[{"name":"ä¸­æ–‡","slug":"ä¸­æ–‡","link":"/categories/%E4%B8%AD%E6%96%87/"},{"name":"Course Lecture Note","slug":"Course-Lecture-Note","link":"/categories/Course-Lecture-Note/"},{"name":"private","slug":"private","link":"/categories/private/"},{"name":"å¿ƒå¾—åˆ†äº«","slug":"ä¸­æ–‡/å¿ƒå¾—åˆ†äº«","link":"/categories/%E4%B8%AD%E6%96%87/%E5%BF%83%E5%BE%97%E5%88%86%E4%BA%AB/"},{"name":"Note","slug":"Note","link":"/categories/Note/"},{"name":"ç­†è¨˜","slug":"ä¸­æ–‡/ç­†è¨˜","link":"/categories/%E4%B8%AD%E6%96%87/%E7%AD%86%E8%A8%98/"}]}