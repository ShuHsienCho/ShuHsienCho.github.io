{"pages":[{"title":"","text":"申请友链须知 原则上只和技术类博客交换，但不包括含有和色情、暴力、政治敏感的网站。 不和剽窃、侵权、无诚信的网站交换，优先和具有原创作品的网站交换。 申请请提供：站点名称、站点链接、站点描述、logo或头像（不要设置防盗链）。 排名不分先后，刷新后重排，更新信息后请留言告知。 会定期清理很久很久不更新的、不符合要求的友链，不再另行通知。 本站不存储友链图片，如果友链图片换了无法更新。图片裂了的会替换成默认图，需要更换的请留言告知。 本站友链信息如下，申请友链前请先添加本站信息： 网站图标：https://removeif.github.io/images/avatar.jpg 网站名称：辣椒の酱 网站地址：https://removeif.github.io 网站简介：后端开发，技术分享 加载中，稍等几秒...","link":"/friend/index.html"},{"title":"","text":"唐艺昕 李沁 李一桐 gakki 图片搜集于互联网，侵权请留言，马上处理😊。","link":"/album/index.html"},{"title":"","text":"Please leave comments if you have any question or idea about the post!","link":"/message/index.html"},{"title":"音乐歌单收藏","text":"--- 温馨提示：选择喜欢的音乐双击播放，由于版权原因部分不能播放。如果喜欢歌单收藏一下，去网易云都能播放哟！","link":"/music/index.html"},{"title":"","text":"About me I am Shu-Hsien Cho (卓書賢), a third-year Ph.D. student studying Biostatistics at University of Texas MD Anderson UTHealth Graduate School of Biomedical Sciences. Before pursuing this Ph.D. degree, I was a research assistant under the supervision of Dr. Yen-Tsung Huang at Institute of Statistical Science (ISS), Academia Sinica. I received my Bachelor degree with double major in Statistics and Economics at National Chengchi University in Jan, 2017. My research interest is genomic data analysis and survival analysis. In addition, I am also interested in causal mediation analysis, and the interaction of survival analysis with other fields. In my research spare time, I am also interested in learning applicable methodologies for baseball data analysis (e.g., bayesian approaches, spatial data, etc.) I also create a baseball data analysis blog (in Mandarin) with my friend. Outside academics, I love baseball, weight training, and photography. Also, I enjoy cooking, wine/beer/whiskey tasting, watching sports game, reading, and sometimes playing contract bridge. In addition to being an outstanding researcher, I hope to keep asking questions and fulfill my curiosity to this world. CV","link":"/about/index.html"},{"title":"","text":"&lt;div class=&quot;music-player&quot;&gt; &lt;div class=&quot;d-title&quot;&gt; &lt;i class=&quot;fa fa-music&quot;&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;听听音乐 &lt;/div&gt; &lt;br/&gt; &lt;/div&gt; &lt;div id=&quot;musicarea&quot;&gt; &lt;div class=&quot;music&quot;&gt;&lt;/div&gt; &lt;p id=&quot;p_message&quot;&gt;&lt;span id=&quot;music_story_message&quot; class=&quot;span_animation&quot;&gt;&lt;/span&gt;&lt;/p&gt; &lt;br/&gt; &lt;ul id=&quot;musiclist&quot;&gt;&lt;/ul&gt; &lt;br/&gt; &lt;div id=&quot;desc&quot;&gt;&lt;/div&gt; &lt;/div&gt; 音乐播放器由mePlayer提供，布局参照网友博客所作，感谢作者的辛勤付出。更多音乐分享请查看歌单。 &lt;div class=&quot;d-title&quot;&gt; &lt;i class=&quot;fa fa-video-camera&quot;&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;看看视频 &lt;/div&gt; &lt;br/&gt; &lt;p class=&quot;hits&quot;&gt;-&gt;点击以下条目开始播放视频,向下滑动查看更多&lt;-&lt;/p&gt; &lt;div id=&quot;video-list&quot;&gt;&lt;/div&gt; &lt;br/&gt; &lt;div id=&quot;dplayer&quot;&gt;&lt;br/&gt;&lt;/div&gt;","link":"/media/index.html"}],"posts":[{"title":"Statistical Inference Note","text":"This lecture note is based on Rice STAT 532: Foundations of Statistical Inference I and Rice STAT 533: Foundations of Statistical Inference II. All the lecture notes will provide an advanced introduction to the mathematical theory of statistics, including comprehensive measure-theoretic probability, common families of distributions, statistical decision theory, statistics, and estimation. Chapter 1: Measure Space File This chapter discusses measures and their definition, construction of functions and induced measures, integration and differentiation, and conditional expectation/distributions. Chapter 2: Probability Measures on Euclidean Spaces File This chapter discusses the detailed properties of probability models on Euclidean spaces, moment inequalities, generating functions, distribution families, and order statistics.","link":"/posts/statinfnote/"},{"title":"Survival Analysis Note","text":"This lecture note is based on UTMDA/GSBS GS01 1023: Survival Analysis. All the lecture notes will provide an introduction to associated theory for the analysis of censored failure time data. Becuase this course mainly discuss the commonly-used methods with heuristic arguments and intuitive explanation of their properties, I will also include my own reading notes on incomplete observations and other types of continuous-time processes such as arise with competing risks data, multivariate failure time data, counting processes approaches, martingale and large-sample properties of estimators and tests, and some possible extensions. Chapter 1: Introduction File","link":"/posts/survivalnote/"},{"title":"MD Anderson GSBS第一學期回顧","text":"在MD Anderson自閉的第一學期即將結束，在參與13-16號之間的ICSA Applied Statistics Symposiums之外，還得在16號考Rice統計推論一的期末考，接下來就是寒假的小休息與reset，在念書之餘想分享一下可能是在這學校最特別的一學期，也給自己一點回顧。 Core Course 這門課可以說是GSBS最重要的課程，也吃掉我最多時間，每天早上都需要上2.5小時的課外，每週也都有2個作業與1個journal club/debate等等。因為是umbrella program的緣故每一週都有不一樣的主題，我想從一個念統計/Quantitative Science的角度來看這門課。 優點 因為要cover每週的主題，這門課找來MD Anderson / UT Medical School / BCM 等地的老師支援開課，對我來說是很新的上課體驗。而且因為不少老師都想收學生，因此上課也可能更認真，或者會推銷自己。此外每週一主題造成內容不會過分深入，相對來說則是更快的節奏，對於毫無基礎的我雖然仍屬辛苦，但是可以想像肯定比純粹的PhD level生物課更易上手。加上這學期其實也有不少內容是拿COVID來作為舉例甚至是當週的journal club / grant proposal主題，對於主題的投入更有感受（但希望我過了這學期不會真的忘光光就好）。此外這堂課的主軸也對bioinfo/statistical genetics的研究方向更為友好！ Translational studies 這門課其實某個程度上證實了這個學校是相對擅長轉譯研究的，很多時候我們都在看癌症相關的target gene以及mechanism/machinery，或者討論特定的pathway，有時候會要求我們基於以上的資訊設想一些有系統的小鼠實驗等等，這相當符合translational study的精神，也讓我了解到將來可能的合作對象是如何問出科學問題，只是並不會延伸到對於統計學家來說更容易協助的Phase 2以後的study，更多像是從bench的研究試圖延伸到病人的研究。 缺點 以生物統計的角度來看，這門課的幫助很有限，在前一個月的課程可以視為Genetic Epidemiology的生物背景知識。每週的journal club/指定閱讀文本也都有包含幾種基本的檢定或者存活分析甚至是視覺化，而這也是biostat/bioinfo課堂都有提及的，只是每週主題跟統計的整合效果不佳，這從每週的biostat作業同學的回答可以感受到。 此外在MDACC，不少的lab需要自己做single cell sequencing，且轉譯醫學本身應該也需要多學科的互相支援，但是這門課並沒有帶到與dry lab合作的必要性，多數在小組活動上的內容也都是以生物科學內容為主，並沒有讓生物背景的學生去嘗試問什麼時候需要量化，於是QS在這堂課中更像是另外一種突兀的存在。 最後則是非戰之罪--互動，這受制於當前疫情，這門課畢竟每天都要看到一樣的人2.5小時到中午，是一個很好讓同學互相交朋友的機會，甚至延伸到日後的合作，只是線上上課大大阻礙了可能性。 Rotation 這應該是對於統計學生來我們學校比較幸福的點，畢竟很少有人可以在一年級就開始做東西。相對於一個禮拜要五個半天的core course，這件事對我來說更熟悉。做了一些存活分析的variable selection同時估計的嘗試與實現，並且試圖用在UK biobank上，接下來希望寒假可以完成大部分的工作並且開始寫Technical report，只是想要同時在私底下多做一點東西的計劃並沒有很成功，可能在修課上多花了不少時間，希望下個學期在修課跟做研究上可以兼顧，並且下學期的rotation是有機會可以自己發展一些先前與老師討論過的想法，讓我更加期待！也要感謝Ryan給了蠻多想法跟建議，在這個特殊的情形與全新的環境中讓我仍然還算順利地活過這學期。 另外，在這裡似乎不用考慮「這真的用得到嗎？」「應用端在意嗎？」，更多時候就是因為有這些資料與現象才會冒出這些問題（或更準確的說，有這些Grant）而能夠更直接感受到該解決的問題。因此我猜想如果是即戰力到MD Anderson來唸書可能會很有收穫，這樣在一開始就可以決定好發展的走向，有研究經驗的幾個同學也是很快速地就把自己的rotation定好，這是對老師或者對學生都很有效率的做法，也可能是他們在入學審資料的時候會看Research statement的緣故，希望日後還能回來驗證這個猜想XD 其他 多虧這裡選課自由，我能跨到Rice去上一些更深入的課程還是不錯（而且還不用多付錢），這樣可以讓訓練更多元又同時有深度，只是還有部分需要改善，特別是WFH的節奏沒有很順利，以及下學期相對更重的課程負擔，還有希望把下兩個rotation做得符合期待，應該會是更大的挑戰。","link":"/posts/mdacc20fall/"},{"title":"Inference on Selected Subgroups in Clinical Trials","text":"This note is based on the X. Guo and X. He (2020), Inference on Selected Subgroups in Clinical Trials, Journal of American Statistical Association Subgroup Analyses and Selection bias Overview Subgroup analyses figure out promising treatments to some subgroups. In practice, subgroup analyses consists of two steps: subgroup identification and subgroup confirmation. In the identification step, researchers looks for the best selected subgroup which comes from domain considerations or data-driven. The existing data-driven identification methods are mostly machine-learning based or model based. The confirmation step provides an inference for possibly additional trials on the promising subgroup. Generally, The inference to the selected subgroup may lead to an overly optimistic evaluation, which is called subgroup selection bias. This kind of bias is originated from the selection mechanism of \\(\\max\\limits_{i=1,\\cdots,k}(\\beta_i)\\). Some of the existing methods for subgroup confirmation might be conservative, without theoretical justification, or model-dependent. Thus, this paper proposes a (data-adaptive) de-biasing bootstrap estimate and a valid one-sided confidence bound. Procedure For predifined subgroups, the interested estimators are the \\(\\beta_s\\) (the true effect size of the subgroup with the largest estimated effect) and \\(\\beta_{\\max}\\) (the largest true subgroup effect size). The procedure is: Obtain the largest modified bootstrapped subgroup effect \\(\\beta^*_{\\max,\\text{modified}}=\\max\\limits_{i=1,\\cdots,k}(\\beta^*_i+d_i)\\) with modification \\(d_i=(1-n^{r-0.5})(\\hat\\beta_{\\max}-\\hat\\beta_i)\\) Construct the bootstrap confidence interval \\(\\hat\\beta_{\\max}\\pm c_\\alpha/\\sqrt{n}\\) with \\(c_{\\alpha}=quantile(\\sqrt{n}(\\max\\limits_{i=1,\\cdots,k}(\\beta^*_{i,b}+d_i)-\\hat\\beta_{\\max}),1-\\alpha)\\) (asymptotically sharp interval for \\(\\beta_s\\)). Derive the bias-reduced estimator \\(\\hat\\beta_{\\max,\\text{reduced}}=\\hat\\beta_{\\max}-E^*[\\beta^*_{\\max,\\text{modified}}-\\hat\\beta_{\\max}]\\) For data-adaptive \\(r\\), tune \\(r\\) with cross validation and bias-reduced estimator. While considering the post-hoc subgroup identification, the procedure is similar to the predefined subgroup identification but no data-adaptive selection for \\(r\\). Possible Extention This method is useful for homogeneous subgroups, because the subgroup selection bias in the homogeneous subgroups might be larger than one in the heterogeneous subgroups. When the subgroup effects are heterogeneous, the naive approach of subgroup identification provide an efficient enough result with less empirical bias. However, the tradeoff among the tuning parameter \\(r\\), heterogeneity, and number of subgroups has not been discussed. With the simulation provided in this paper, the bias and coverage get worse when the number of subgroups or \\(r\\) gets large. When \\(r\\) gets large, it means that the effect modification \\(d\\) goes to 0. We could also discover that the empirical bias with small \\(r\\) performs better than large \\(r\\) in homogeneous subgroups setting. However, this situation is reversed in heterogeneous setting. Thus, the relationship between modification, heterogeneity, and number of subgroups might be a potential issue to work on. Another possible extension associates with the confounding adjustment, especially for trial data and observational studies. This method allows overlapped subgroup; that is, we cannot think the subgroup as the stratification. With confounding adjustment, the subgroup effect size should be reconsidered.","link":"/posts/issct/"},{"title":"Encrypted article test","text":"4630436162ade97ba2718b7d0c4b3b63480c11f21f943edd569d93952dd2b42892dfbf6fb485b2d42bb78b95cad3282d1005bfd10618e6a253f3ffed39a7c6667e0ba01d51487801aba32262afb3e79ca37c3088bcd1bcdee4f5705b22865d319380a980be050100a4bbd113d31bcdc00e1997d2d9632516acb06ba8a15d633ffe77e3ba1cb4ab84175db72ef02bc1fa9b5916f45bdf66fe29e9d5718bf1e717431ef44d8a617418b5e46b870e4d0ea43362263ef1619f83e3645c2d889b9e68707039f5f38a6ee9383474f800b64a0a60ffb1659bf7e9e3b3824b028875513694bdf317a5aac0c516f34bf11ec567650887ba988a9528037268a62d08bd95f042da6b38e27f3dd8d1ea4e133673a54e498ee57697cd7ce72351ec514cfbde61cb98df28671d1bf5b677f68afb1cadd5bcee0e378e10c242d98a4ccb3b11e0c047c17a56733c07e60644c9d1a36b6a0da940acbf6f120439aa59be38d3d4224749b451fad57c5f7e9cf5cba4f73ff697515a34c3e46d799fff5e3d3ea6737a1e06e71123c5f6d1ac031e7a6f4c50e5537bfd9adb2fe50bf7060706c0c0520d5a68f4752e754403eb269acf37b8c685615a274a56841c80c6b6cd6face6baef0625a2d751b5860455bf13418cb3b9595e085349af9a2b2d61c1faf425c65f494df2786c8fb2eff30daa7a0376c930d1eb12ebbb0462401b4c6a61af7860fca2159abc06523546fd7ee64013443d52aaf13df6f6c8dbf375fcd769ecde26ef885d0cda0899ede643026ad3881e53cc36be999d0d5c937d89da81f70dac909d4663db7a131edf4458c3e91b2cf310371d29cd102874daa97c6e8ac02a8d000467fd981989a4c8e6f71d8fe46a5e946adaa8dd823f011f84192292b9b6321cddab04d5e4ca11d32293d7479b1ada515121a76982e460b9ec5aa45d50e37ab15c0c50 Please try the password 123456.","link":"/posts/7e709353/"},{"title":"因果推論筆記（一）：What is Causality?","text":"Intro 有鑒於過去在中研院擔任RA時，並沒有建立良好的習慣來記錄自己三年來的所學，而這個網站為了培養這個習慣之餘，我也希望能一點點的把過去所學建檔，並且在未來能有所用，因此接下來會有一系列我個人對於因果推論的摘要，以及因果中介分析論文的內容介紹。 當然，近來因果推論在AI的領域相當盛行，而對這領域有些了解的人必然知道，事實上在40年前就已經開始有電腦科學家在進行因果推論的研究，而他們是使用一種稱為Directed Acyclic Graphs (DAG) 的模式來研究AI，而其中最著名的人就屬在UCLA資工系任教的Dr. Judea Pearl。但在流行病學與統計學領域中也有兩位相當著名的教授，分別是Dr. James Robins 以及Dr. Don Rubin兩位在Harvard的老師（甚至兩位老師的觀點也有些不同），而接下來的介紹會更接近流行病學與統計學中使用的因果推論為主，因此內容會包含較多的符號與統計概念，如果對資工以及AI有興趣的人可以去閱讀Dr. Judea Pearl所寫的“Causality”，或者是更為通俗的“因果革命”。 Overview 在因果推論中，了解因果之前必須要先了解因果關係、相關、以及交互作用的差異，了解這些就能對不直觀的矛盾找出更合理的解釋，如Simpson Paradox。當我們有了清楚的分界，接下來就能將因果推論擴展到最常用的兩個主題：隨機試驗以及觀察性研究，並且介紹因果推論在這兩個主題上的各種應用，包含前述的DAG，以及經常出現的選擇偏誤跟干擾因子在因果推論中的處理等等。界定好上述問題後，可以將因果推論擴展到因果中介分析，以及在經濟學、政治學以及遺傳學中常見的工具變量 (Instrumental Variable，在遺傳學中則為Mendelian Randomization)，最後則會基於因果中介分析來介紹一些閱讀過或者是我有參與的論文。 What is Causality? 人生而為人永遠都在尋找因與果，不管是俗諺中的「種什麼因，得什麼果」、佛教信仰中的「因果輪迴」、乃至於今天學習科學中的「因果關係」，再再證明人類對因果推論的本能與渴求，可大多數時候我們不求甚解，經常性地將僅僅是眼睛看見的資訊（亦即統計上的observation）得到的關係作為因果，造成了統計上的名言：「相關並不代表因果」，然而我們卻缺乏適當的數學工具在統計中描述因果並解釋之。雖然我們可以透過日常的事件來思考甚至回答兩件事情是否為因果，例如：疾病治療的效果、冰淇淋銷量與鯊魚攻擊數量的正相關、銷售額的增加要歸因於廣告還是價格等，事實上想要在數學上建立因果事實上需要更多的條件，如藥廠在進行藥物開發與療效評估時，必須要創造出隨機控制試驗 (Randomized Control Trial) 這樣近乎完美的試驗，甚至還得複製數次才得以找出因果關係，而這件事恰恰說明因果推論的核心觀念：控制所有變因下的可複製結果 想要創造出這樣完美的實驗，我們必須要先有「反事實架構 (Counterfactual framework)」 以及「介入 (Intervention)」，counterfactual以中文常見的語彙來說是「如果做了什麼『介入』，會怎樣？」也就是我們創造了另外一個平行世界來推論，然後透過intervention來推論因果是否存在。例如：雖然我們都說雞啼之後太陽就會升起，但我們可以直覺得知在另外一個平行世界中，即使今天把雞燉成雞湯，太陽依然照常升起。我們也能從這樣的例子知道，因果關係同時也潛藏時間的先後順序性，以及因-&gt;果這樣的一個箭頭方向。為什麼生物統計、醫學、流行病學、試驗等等方法論牽涉更多的因果推論？因為人只有一條生命與一個歷程，我們沒有辦法在現實的觀察中得到另外一個一模一樣的你跟我，加上在醫學的研究中不能出差錯，畢竟人死不能復生，所以在這方面就需要更嚴謹的因果推論架構，不像做機器學習可以更有彈性的調整參數而不對真實人命有影響（也不會被告），事實上任何的預測方法其實都未必需要因果推論，大部分現存的機器學習方法也很難辯證干擾因子以及偏誤。 當我們知道了因果推論的重要性之後，下一步則討論如何結合因果推論與統計推論。首先回到統計的觀念，進行推論之前我們必須要有目標的estimand，這個待估計的量可以讓我們用估計量 (estimator) 來近似我們想從資料中推斷出的母體資訊，但在創造estimand時，比起使用機率與模型，我們更多時候是優先考慮找到一個想得到的資訊的表現形式（如我們想衡量療法能降低多少死亡風險），再去使用模型來想辦法得到estimator這個表現形式（如log hazard ratio），才得到估計值 (estimate) 這個近似值，最後就是把因果的直觀與統計推論連接，並且在有共識的反事實框架下有效地透過因果來溝通。因此，在定義問題與闡釋因果的階段，我們必須在考慮資料特性前就先評估因果，而不是優先考慮可用模型，並且適當的使用一些無須考慮資料特性的因果假設來創造一個前述的平行世界，這些假設我其實很常稱呼為多拉A夢的時光機，而這些假設會在後面提到。 Conclusion 綜上所述，我們必須了解到因果推論是發生在統計推論之前，並且我們不會因為有了Causal inference讓我們的統計推論得到天翻地覆的變化或是發展出全新的統計量成為causal一方之霸，也不是因為理解causal而開嘴任何的統計方法都沒有因果關係或不符合假設，畢竟我們如果把研究放到量子糾纏的層次，永遠都沒辦法排除掉那隱含的不確定性。因果推論的科學問題更著重在確保怎樣的條件與假設下，使得因果成立，並且得到我們有興趣且具有因果關係的標的，我想這才是因果推論的精髓。 (To be continued)","link":"/posts/causal1/"},{"title":"因果推論筆記（二）：Causal Effect","text":"Intro 上一篇筆記中簡單敘述因果推論的重要性，以及與統計推論結合如何實現，這份筆記將會從數學符號起手，來介紹counterfactual的架構，並且得到causal effect。在開始之前先舉個實際上的例子：我們想要評估炎性乳腺癌 (Inflammatory breast cancer)的病患中，如果施行三合一療法 (\\(S\\))後，存活與否 (\\(Y\\)) 的差別。下面將會以這個舉例貫穿全文。 Counterfactual notation and Average causal effect 先回到統計上的符號，在統計中通常大寫代表了隨機變數 (random variables)，因此我們在建構這些符號時，也是以隨機變數的精神來思考。首先我們會有兩個平行宇宙，在第一個宇宙中某病人施行了三合一療法，被我們標記為\\(Y_i(s=1)\\)，即使是在這個施行三合一療法的宇宙中，病人是否存活仍然存在著不確定性，亦即\\(Y_i(s=1)\\)也是一個隨機變數，我們會以一個機率模型描述他；同理，在另一個平行宇宙中該病人並未施行三合一療法，我們將其標為另一個隨機變數\\(Y_i(s=0)\\)，若\\(Y_i(s=1)-Y_i(s=0)\\neq 0\\)，即是有individual causal effect，而這裡的\\(Y(s)\\)，通常稱之為counterfactual outcome或是potential outcome。然而，我們僅有接受治療或未接受治療中的一個情況發生，若某病人實際上接受治療 (\\(S\\)=1)，則counterfactual outcome中 \\(Y_i(s=1)=Y_i|(S=1)\\)（e.g. 具有相同的p.d.f.），也就是我們觀察到的outcome等於counterfactual outcome，這性質我們稱之為causal consistency，以白話文來敘述就是如果某病人在物理世界中接受某個治療，平行宇宙中同樣接受治療的該病人會具有相同的歷程。 由於不可能在現實中找到同一個人同時接受與不接受治療，因此\\(Y_i(s=1)\\)與\\(Y_i(s=0)\\)必然有一個是觀察不到(missing data，同樣的因果推論也是在處理missiing data的問題！) ，那就無法得到這個病人的individual causal effect，此處我們需要Average causal effect來協助我們找到群體治療與否的影響，也就開始會有機率與統計的介入，此處除了一樣需要randomization來確保兩個不同治療之下的病人群體是類似的之外，也需要這個群體足夠大以滿足大樣本性質來幫助我們找到類似的人。Average causal effect的定義相當簡單，由於我們推論的是群體資訊，因此average causal effect的關係式為\\(P[Y(s=1)]\\neq P[Y(s=0)]\\)，又或者是\\(E[Y(s=1)]\\neq E[Y(s=0)]\\)，更廣義的來說，我們也可以用中位數或者是empirical distribbution來定義average causal effect，只要causal effect是兩個不同counterfactual outcome各自的marginal distribution的函數相互比較即可。 Causal effect measure and Association measure 有了前述的關係式，我們得以創造因果推論上的假說檢定\\(H_0: P[Y(s=1)=1]= P[Y(s=0)=1]\\)，方便我們量測average causal effect的estimand有以下幾種： \\[\\begin{cases} \\text{Causal risk difference:} &amp;P[Y(s=1)=1]- P[Y(s=0)=1]=0\\\\ \\text{Causal risk ratio:} &amp;\\dfrac{P[Y(s=1)=1]}{P[Y(s=0)=1]}=1\\\\ \\text{Causal odds ratio:} &amp;\\dfrac{P[Y(s=1)=1]/P[Y(s=1)=0]}{P[Y(s=0)=1]/P[Y(s=0)=0]}=1\\\\ \\text{Number needed to treat:} &amp;NNT=\\dfrac{-1}{P[Y(s=1)=1]- P[Y(s=0)=1]} \\end{cases}\\] 然而，這跟平常所學的似乎不太一樣？考慮association的上述estimands可以表示為： \\[\\begin{cases} \\text{risk difference:} &amp;P[Y=1|S=1]- P[Y=1|S=0]=0\\\\ \\text{risk ratio:} &amp;\\dfrac{P[Y=1|S=1]}{P[Y=1|S=0]}=1\\\\ \\text{odds ratio:} &amp;\\dfrac{P[Y=1|S=1]/P[Y=0|S=1]}{P[Y=1|S=0]/P[Y=0|S=0]}=1 \\end{cases}\\] 以下借用過去的投影片來描述這兩者有何不同之處 物理世界中我們能觀察到的資料就是左邊的方形，我們可以看到如果是相關性，我們是在比較觀察到不同\\(S\\)條件下個別的\\(Y\\)，而因果則是將相關性比較中缺漏的部分補成完整的方形，也就是在比較如果整個群體在不同\\(S\\)條件下的\\(Y\\) 因此若是觀察到的資料會因為\\(S\\)的關係而產生不一樣的分配（如方形中的藍點），很顯然能發現相關性在此刻不等於因果 而\\(S\\)若對於藍點的分配並無影響，也就是如果沒有干擾因子，則相關性就能等於因果！ (To be continued)","link":"/posts/causal2/"},{"title":"Implementations of the Monte Carlo EM Algorithm","text":"This note is based on the R. Levine and G. Casella (2001), Implementations of the Monte Carlo EM Algorithm, Journal of Computational and Graphical Statistics Monte Carlo EM Algorithm Overview MCEM is a modification of the EM algorithm where the conditional expectation of log-likelihood in the E-step is computed numerically through Monte Carlo simulations, and MCMC sampler (Gibbs Sampler / Metropolis-Hasting) is the most flexible approach for Monte Carlo sample. Target Minimizing computational cost from non-closed form E-step (Problem from Wei(1990) original paper) to approximate the posterior: importance sampling whereby samples drawn during previous EM iterations are recycled Choosing sample size: regenerate approximate samples by subsampling the generated MCMC sample during different renewal periods. Existing paper Booth an Hobert(1990): Gauge error by increase sample size as algorithm converges, but requires i.i.d. samples or importance weighted. MCMC: Though the random variates are dependent in such a scenario, the E-step estimator is still unbiased and approaches the true value as the sample size increases. Notation Observed data: \\(\\mathbf{y} = (y_1,\\cdots,y_n)^T\\) Parameters: \\(\\mathbf{\\Psi}\\) MLE is simpler to compute on the data augmented by a set of latent variables \\(\\mathbf{u}=(u_1,\\cdots,u_q)^T\\) conditional distribution of the latent variables: \\(g\\left(\\mathbf{u}|\\mathbf{y,\\Psi^{(r)}}\\right)\\) with iteration \\(r\\) EM: Complete data likelihood: \\(Q\\left(\\mathbf{\\Psi}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)=E_{\\hat{\\mathbf{\\Psi}}^{(r)}}\\{\\text{ln} f(\\mathbf{y,u}|\\mathbf{\\Psi})|\\mathbf{y}\\}=\\int\\text{ln} f(\\mathbf{y,u}|\\mathbf{\\Psi})g\\left(\\mathbf{u}|\\mathbf{y,\\Psi^{(r)}}\\right)d\\mathbf{u}\\) and maximize it over \\(\\mathbf{\\Psi}\\) Obtain a sample \\(\\mathbf{u^{(r)}_1},\\cdots,\\mathbf{u^{(r)}_m}\\) from conditional distribution, the expectation can be estimated by Monte Carlo sum \\(Q\\left(\\mathbf{\\Psi}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)=\\dfrac{1}{m}\\sum\\limits^m_{t=1}\\text{ln}f\\left(\\mathbf{y,u^{(r)}_t|\\Psi}\\right)\\) \\(m\\) denotes the dependence of this estimator on the MC sample size. So, how could we obtain a random sample from \\(g\\left(\\mathbf{u}|\\mathbf{y,\\Psi}\\right)\\) , and how do we choose \\(m\\)? Importance Sampling Drawing an MCMC sample each iteration of the EM algorithm could be prohibitively costly particularly for large \\(m\\). Importance weight \\(w_t=\\dfrac{g\\left(\\mathbf{u}_t|\\mathbf{y,\\Psi^{(r)}}\\right)}{g\\left(\\mathbf{u}_t|\\mathbf{y,\\Psi^{(0)}}\\right)}=\\dfrac{L(\\mathbf{\\hat{\\Psi}^{(r)}|u_t,y}))/L(\\mathbf{\\hat{\\Psi}^{(r)}|y})}{L(\\mathbf{\\hat{\\Psi}^{(0)}|u_t,y})/L(\\mathbf{\\hat{\\Psi}^{(0)}|y})}\\) corrects the original sample \\(\\mathbf{u}\\) with the new information we have at iteration \\(r\\) How much expense do we save? The expense saving with respect to the weights are not dependent on the unknown likelihood \\(L(\\mathbf{\\Psi|y})\\) because it would be cancelled Reasonable choice: \\(Q_m\\left(\\mathbf{\\Psi}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)=\\dfrac{\\sum\\limits^m_{t=1}w_t\\text{ln}f\\left(\\mathbf{y,u^{(r)}_t|\\Psi}\\right)}{\\sum\\limits^m_{i=1}w_t}=\\dfrac{\\sum\\limits^m_{t=1}w'_t\\text{ln}f\\left(\\mathbf{y,u_t|\\Psi}\\right)}{\\sum\\limits^m_{i=1}w'_t}\\) \\(w'_t=\\dfrac{L(\\mathbf{\\hat{\\Psi}^{(r)}|u_t,y})}{L(\\mathbf{\\Psi^{(0)}|u_t,y})}\\) The reasonable choice has smaller MSE than the below one (Liu 1996 and Casella 1998). Another choice of important sampling: \\(Q_m\\left(\\mathbf{\\Psi}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)=\\dfrac{1}{m}\\sum\\limits^m_{t=1}w_t\\text{ln}f\\left(\\mathbf{y,u_t|\\Psi}\\right)\\). This would not affect the EM because the normalizing constant \\(\\dfrac{L(\\mathbf{\\Psi^{(0)}|y})}{L(\\mathbf{\\hat{\\Psi}^{(r)}|y})}\\) only depends on initial value and updated estimate with iteration \\(r\\) and does not involve the maximization. Use the reasonable one could avoid calculation of normalizing constant when choosing m. Burn-in in first few iteration: This approach fails when the estimates with \\(r\\) iteration is too far from the initial value. The target density would be close enough to have small variance. Burn-in time is problem specific depending on how close we need to be to the MLE in order to obtain stable importance weights in subsequent M-steps. Trial runs of the EM algorithm with importance sampling, gauging the variability in the importance weights as the algorithm converges, will provide the user with an idea for the appropriate burn-in time. Algorithm Initialize \\(m\\) and \\(\\mathbf{\\Psi}^{(0)}\\) Generate \\(\\mathbf{u_1},\\cdots,\\mathbf{u_m}\\sim g\\left(\\mathbf{u}|\\mathbf{y,\\Psi^{(0)}}\\right)\\) via MCMC At iteration \\(r+1\\), compute the importance weight \\(w_t=\\dfrac{L(\\mathbf{\\hat{\\Psi}^{(r)}|u_t,y})}{L(\\mathbf{\\Psi^{(0)}|u_t,y})}\\) E-step: estimate \\(Q\\left(\\mathbf{\\Psi}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)\\) by \\(Q_m\\left(\\mathbf{\\Psi}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)=\\dfrac{\\sum\\limits^m_{t=1}w_t\\text{ln}f\\left(\\mathbf{y,u_t|\\Psi}\\right)}{\\sum\\limits^m_{i=1}w_t}\\) M-step: \\(\\max\\limits_{\\Psi}Q_m\\left(\\mathbf{\\Psi}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)\\) to obtain \\(\\hat{\\mathbf{\\Psi}}^{(r+1)}\\) MC error estimation: Compute for each \\(j=1,\\cdots,s\\), \\(\\hat\\mu_{m;j}=\\sum\\limits^m_{t=1}w_t\\dfrac{\\partial}{\\partial \\psi(j)}\\text{ln}f\\left(\\mathbf{y,u_t|\\Psi}\\right)/\\sum\\limits^m_{t=1}w_t\\) by plugging in \\(\\mathbf{\\Psi}=\\hat{\\mathbf{\\Psi}}^{(r+1)}\\) Compute for each \\(j=1,\\cdots,s\\), \\(\\hat{v}_{m;j}=\\sum\\limits^m_{t=1}w_t\\left[\\dfrac{\\partial}{\\partial \\psi(j)}\\text{ln}f\\left(\\mathbf{y,u_t|\\Psi}\\right)\\right]^2/\\left(\\sum\\limits^m_{t=1}w_t-\\hat\\mu_{m;j}^2\\right)\\) by plugging in \\(\\mathbf{\\Psi}=\\hat{\\mathbf{\\Psi}}^{(r+1)}\\) Obtain the \\((1-\\alpha)\\) confidence interval about \\(Q^{(1)}_j\\left(\\mathbf{\\Psi}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)\\) by the above mean and variance from standard normal distribution. Obtain subsampling instants \\(t_k=x_1+\\cdots+x_k\\) where \\(x_k-1\\sim Poi(\\nu_k), k=1,\\cdots,N\\) and \\(N=\\sup\\{n;t_n\\leq m\\}\\) If \\(Q^{(1)}_m\\left(\\hat{\\mathbf{\\Psi}}^{(r)}|\\mathbf{\\Psi}^{(r-1)}\\right)\\) lies in confidence interval, then Set \\(m_o=m\\) Set \\(m=m_o+gauss(m_o+c)\\) for some \\(c&gt;0\\) Obtain \\(\\mathbf{u_{m_o+1}},\\cdots,\\mathbf{u_m}\\sim g\\left(\\mathbf{u}|\\mathbf{y,\\Psi^{(0)}}\\right)\\) Compute for each \\(j=1,\\cdots,s\\), \\(Q^{(1)}_{m;j}\\left(\\hat{\\mathbf{\\Psi}}^{(r+1)}|\\hat{\\mathbf{\\Psi}}^{(r)}\\right)=\\sum\\limits^N_{k=1}w_{t_k}\\dfrac{\\partial}{\\partial \\psi(j)}\\text{ln}f\\left(\\mathbf{y,u_{t_k}|\\Psi}\\right)/\\sum\\limits^m_{t=1}w_{t_k}\\) by plugging in \\(\\mathbf{\\Psi}=\\hat{\\mathbf{\\Psi}}^{(r+1)}\\) Repeat step 3 to step 9 until converge Burn-in steps run burn-in for one minutes Set importance weights \\(w_t = 1, \\forall t=1,\\cdots,m\\) at iteration \\(b\\) Generate \\(\\mathbf{u_1},\\cdots,\\mathbf{u_m}\\sim g\\left(\\mathbf{u}|\\mathbf{y,\\Psi^{(b)}}\\right)\\) via MCMC Run E and M step above with \\(r=b\\) Repeat Steps 2 and 3 for \\(B\\) burn-in iterations. Reinitialize \\(\\mathbf{\\Psi}^{(0)}=\\mathbf{\\Psi}^{(B)}\\)","link":"/posts/imcem/"}],"tags":[{"name":"Lecture Note","slug":"Lecture-Note","link":"/tags/Lecture-Note/"},{"name":"心得","slug":"心得","link":"/tags/%E5%BF%83%E5%BE%97/"},{"name":"Note","slug":"Note","link":"/tags/Note/"},{"name":"Subgroup Analysis","slug":"Subgroup-Analysis","link":"/tags/Subgroup-Analysis/"},{"name":"Clinical Trial","slug":"Clinical-Trial","link":"/tags/Clinical-Trial/"},{"name":"Encrypted","slug":"Encrypted","link":"/tags/Encrypted/"},{"name":"Causal","slug":"Causal","link":"/tags/Causal/"},{"name":"EM Algorithm","slug":"EM-Algorithm","link":"/tags/EM-Algorithm/"},{"name":"Monte Carlo","slug":"Monte-Carlo","link":"/tags/Monte-Carlo/"},{"name":"DeMiX-NB","slug":"DeMiX-NB","link":"/tags/DeMiX-NB/"}],"categories":[{"name":"Course Lecture Note","slug":"Course-Lecture-Note","link":"/categories/Course-Lecture-Note/"},{"name":"中文","slug":"中文","link":"/categories/%E4%B8%AD%E6%96%87/"},{"name":"Note","slug":"Note","link":"/categories/Note/"},{"name":"private","slug":"private","link":"/categories/private/"},{"name":"心得分享","slug":"中文/心得分享","link":"/categories/%E4%B8%AD%E6%96%87/%E5%BF%83%E5%BE%97%E5%88%86%E4%BA%AB/"},{"name":"筆記","slug":"中文/筆記","link":"/categories/%E4%B8%AD%E6%96%87/%E7%AD%86%E8%A8%98/"}]}